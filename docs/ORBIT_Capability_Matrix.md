### ORBIT AI Capability Matrix

ORBIT is an open-source, AI orchestration platform designed for data privacy, security, and scalability. It enables organizations to build powerful, context-aware AI applications by connecting their own data and models.

| Feature Area | Capability | Description | Business Value |
| :--- | :--- | :--- | :--- |
| **AI & Retrieval** | **Intent-Based SQL (Text-to-SQL)** | Translates natural language questions (e.g., "Show me last month's sales in Canada") into executable SQL queries using a semantic matching engine. Supports complex analytics. | **Democratizes Data Access:** Allows non-technical business users to query complex databases in any language, reducing reliance on BI teams and accelerating data-driven decisions. |
| | **Retrieval-Augmented Generation (RAG)** | Enhances LLM responses by retrieving relevant information from your organization's private data sources, including vector, SQL, and NoSQL databases (multi-modal coming soon). | **Reduces Hallucinations:** Grounds AI responses in factual, proprietary data, ensuring accuracy and relevance. Unlocks the value of existing enterprise knowledge bases. |
| | **Question-Answering (QA)** | Provides direct answers from structured (SQL) and unstructured (vector DB) knowledge bases using semantic search and confidence scoring. | **Improves Efficiency:** Delivers fast, accurate answers to common questions, automating support, and improving knowledge worker productivity. |
| | **Dynamic Conversation Memory** | Automatically manages conversation history size based on the active AI model's context window, from small local models to large-scale cloud APIs. | **Optimal Performance & Context:** Maximizes conversational context without manual tuning, preventing errors and ensuring a seamless user experience across different models. |
| **Connectivity & Integration** | **Provider-Agnostic LLM Support** | Integrates with over 20 leading LLM providers, including local (Ollama, Llama.cpp), open-source (HuggingFace), and commercial APIs (OpenAI, Anthropic, Google Gemini, AWS, Azure). | **Future-Proofs Your AI Strategy:** Avoids vendor lock-in and allows you to switch models and providers at any time to optimize for cost, performance, or capability without re-architecting. |
| | **Broad Data Source Integration** | Connects to a wide array of enterprise data sources, including SQL (Postgres, MySQL, SQLite), Vector DBs (Chroma, Qdrant, Pinecone, Milvus), and others (Elasticsearch, Redis, MongoDB). | **Leverages Existing Infrastructure:** Integrates directly with your current data ecosystem, eliminating the need for costly data migration projects. |
| | **Embeddable Web Chat Widget** | A customizable, pre-built chatbot widget that integrates into any website. Supports floating/embedded modes, extensive theming (colors, icons), suggested questions, and includes a live-theming app for developers. | **Rapid Customer-Facing Deployment:** Provides a turnkey solution for deploying a branded chatbot for customer support or lead generation, drastically reducing frontend development time. |
| | **Node.js/TypeScript Client Library** | A published NPM package (`@schmitech/chatbot-api`) for seamless integration with any JavaScript-based application (Node.js, React, React Native, etc.), supporting streaming and session management. | **Accelerates Custom Development:** Enables developers to quickly build custom web and mobile applications on top of the ORBIT platform with a ready-to-use client. |
| | **Python Client & CLI** | A pip-installable Python package (`schmitech-orbit-client`) providing both a library for programmatic integration and an `orbit-chat` command-line tool for direct interaction and testing. | **Python Ecosystem:** Allows Python developers and data scientists to easily integrate ORBIT into their applications, scripts, and data workflows. |
| | **Extensible Adapter Architecture** | A modular, configuration-driven system allows for adding new data sources, LLMs, and business logic without changing core code. Adapters are defined in simple YAML files. | **Agility & Low Maintenance:** Rapidly develop and deploy new AI capabilities. The clean separation of concerns simplifies maintenance and reduces development overhead. |
| **Security & Governance** | **Content Safety (LLM Guard)** | Provides a dedicated security layer that scans both user prompts and AI responses for threats like prompt injection, toxicity, secrets, and sensitive data exposure. | **Mitigates AI-Specific Risks:** Protects the organization from malicious inputs and prevents the AI from leaking confidential information, ensuring safe and compliant AI interactions. |
| | **Content Moderation** | Integrates with multiple content moderation providers (OpenAI, Anthropic, Ollama) to filter and flag inappropriate or harmful content in real-time. | **Enforces Brand Safety:** Ensures all AI-generated content aligns with company policies and ethical guidelines, protecting brand reputation. |
| | **Authentication & Authorization** | Features a robust authentication system with secure password hashing (PBKDF2-SHA256), role-based access control (RBAC), and CLI-based user management. | **Secure & Controlled Access:** Enforces security standards, ensuring that only authorized users can access and manage the platform according to their defined roles. |
| | **API Key Management** | Provides tools for creating, managing, and revoking API keys, which can be associated with specific adapters and system prompts for scoped access. | **Granular Access Control:** Enables secure, controlled access for various applications and services, with the ability to manage permissions and monitor usage. |
| **Platform & Architecture** | **High-Performance Concurrency** | Built with a multi-worker architecture and specialized thread pools for I/O, CPU, and inference tasks, designed to handle thousands of concurrent requests. | **Enterprise Scalability:** Reliably serves a large user base and handles high-volume workloads, ensuring the platform can grow with your business needs. |
| | **Fault Tolerance** | Features a circuit breaker pattern with per-adapter health monitoring, automatic recovery, and exponential backoff. It supports parallel, non-blocking execution of adapters. | **High Availability & Resilience:** Automatically isolates failing services to prevent cascading failures, ensuring the platform remains stable and responsive even during partial outages. |
| | **Composable Inference Pipeline** | Processes requests through a series of discrete, observable steps (Safety, Language Detection, Retrieval, Inference, Validation), which are configured automatically. | **Flexibility & Observability:** Provides a clear, modular, and easily debuggable inference process. New capabilities can be added as steps without disrupting the system. |
| **Operations & Management** | **Containerized Deployment (Docker)** | Provides a Docker Compose setup with helper scripts for one-command initialization, configuration, and management of the entire platform stack (server, DBs, cache). | **Simplified & Reproducible Deployments:** Reduces setup time and ensures consistency across development, staging, and production environments. |
| | **Configuration** | Centralized and modular YAML-based configuration for all aspects of the platform, including adapters, data sources, models, and security settings, with support for environment variables. | **Simplified Management:** Allows for easy and transparent management of the entire platform. Configuration-as-code enables version control and repeatable deployments. |
| | **Full-Featured CLI** | A command-line interface (`orbit.sh`) for managing the server, users, API keys, and system prompts. | **Automates Administration:** Simplifies server operations and administrative tasks, enabling automation and integration with existing DevOps workflows. |
| | **Real-Time Monitoring & Logging** | Provides detailed logging, health check endpoints, and real-time performance metrics for all components, including circuit breakers and thread pools. | **Operational Insights:** Enables proactive monitoring, rapid troubleshooting, and performance tuning, reducing downtime and ensuring operational continuity. |
| **Quality Assurance** | **Test Coverage** | Test suites to validate core services, adapter pipelines, safety layers, and data integrations alongside CLI and performance tests to catch regressions early. | **Release Stability** Maintains confidence in releases by detecting defects before deployment, reducing operational risk and preserving SLA. |
