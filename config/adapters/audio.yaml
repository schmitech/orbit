# Audio Transcription Adapters
# Specialized adapters for transcribing audio files

adapters:
  # Audio transcription adapter - specialized for transcribing audio files
  - name: "audio-transcription"
    enabled: true
    type: "retriever"
    datasource: "none"
    adapter: "file"
    implementation: "retrievers.implementations.file.file_retriever.FileVectorRetriever"
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    
    # Provider overrides
    audio_provider: "gemini"    # Use Gemini for transcription (changed from openai)
    embedding_provider: "openai"
    
    # Capabilities: Audio file transcription
    capabilities:
      retrieval_behavior: "conditional"
      formatting_style: "clean"
      supports_file_ids: true
      supports_session_tracking: false
      requires_api_key_validation: true
      skip_when_no_files: true
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "transcription_language"  # Optional: specify language
    
    config:
      # Storage configuration
      storage_backend: "filesystem"
      storage_root: "./uploads"
      max_file_size: 104857600  # 100MB for audio files
      
      # Audio-specific settings
      enable_audio_transcription: true
      transcription_language: null  # Auto-detect, or "en-US", "fr-FR", etc.
      transcription_format: "text"   # Return format: "text", "json", "srt", "vtt"
      
      # Supported audio formats
      supported_types:
        - "audio/wav"
        - "audio/mpeg"
        - "audio/mp3"
        - "audio/mp4"
        - "audio/ogg"
        - "audio/flac"
        - "audio/webm"
      
      # Vector store for transcribed text
      vector_store: "chroma"
      collection_prefix: "audio_transcriptions_"
      
      chunking_strategy: "recursive"
      chunk_size: 2000  # Larger chunks for transcriptions
      chunk_overlap: 200
      
      confidence_threshold: 0.3
      max_results: 10
      return_results: 5

# Voice chat adapter - accepts audio input and returns audio responses
  - name: "voice-chat"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"
    
    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "elevenlabs"  # For both STT and TTS
    
    # Capabilities: Voice-enabled chat
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "audio_input"      # Base64-encoded audio data
        - "audio_format"    # Input audio format (mp3, wav, etc.)
        - "language"        # Language code for STT
        - "session_id"
        - "return_audio"     # Whether to return audio response
        - "tts_voice"        # Voice for TTS (alloy, echo, fable, etc.)
    
    config:
      # Audio processing settings
      audio_input_enabled: true
      audio_output_enabled: true
      stt_language: "en-US"     # Default language for speech-to-text
      # tts_voice: "despina"      # Default voice
      tts_format: "wav"         # Default output format (Gemini uses wav)
      return_text: true         # Also return text alongside audio
      return_audio: true        # Return audio response

  # Multilingual voice assistant - supports translation and voice I/O
  - name: "multilingual-voice-assistant"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"
    
    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "gemini"
    
    # Capabilities: Multilingual voice assistant
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "audio_input"
        - "source_language"    # Language of input audio
        - "target_language"    # Language for output audio
        - "session_id"
    
    config:
      # Language settings
      default_source_language: "auto"  # Auto-detect input language
      default_target_language: "en-US" # Default output language
      
      # Google TTS settings (multilingual)
      tts_model: "neural2"
      tts_voice: "en-US-Neural2-A"
      tts_language_code: "en-US"
      tts_audio_encoding: "MP3"
      
      # Google STT settings
      stt_model: "latest_long"
      stt_language_code: "en-US"
      stt_sample_rate: 16000
      stt_encoding: "LINEAR16"
      
      # Audio I/O
      audio_input_enabled: true
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Premium voice chat with ElevenLabs (high-quality TTS)
  - name: "premium-voice-chat"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "elevenlabs"  # High-quality TTS
    # Note: ElevenLabs is TTS-only, STT would need separate provider
    # For now, this adapter expects text input (can be extended)

    # Capabilities: Premium voice output
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "voice_id"           # ElevenLabs voice ID
        - "session_id"
        - "return_audio"       # Return audio response

    config:
      # ElevenLabs TTS settings
      tts_model: "eleven_multilingual_v2"
      tts_voice: "5opxviIE64D8KxYYJKpx"
      tts_format: "mp3_44100_128"
      tts_stability: 0.5
      tts_similarity_boost: 0.75
      tts_style: 0.0
      tts_use_speaker_boost: true

      # Audio output
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Hybrid local voice chat - Whisper STT + vLLM TTS (fully local, no API costs!)
  - name: "vllm-voice-chat-hybrid"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "vllm"  # vLLM for TTS (Orpheus model) - local, free
    # NOTE: For STT, client should send text or use separate Whisper processing
    # This adapter focuses on TTS output using local vLLM server

    # Capabilities: Voice output with local TTS
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "session_id"
        - "return_audio"
        - "tts_voice"

    config:
      # vLLM TTS settings (local, no API key needed)
      tts_model: "canopylabs/orpheus-3b-0.1-ft"
      tts_voice: "tara"  # Options: tara, leah, jess, leo, dan, mia, zac, zoe
      tts_format: "mp3"

      # Audio output
      audio_input_enabled: false  # Use Whisper separately for STT
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Coqui TTS voice chat - Local, open-source text-to-speech (no API costs!)
  # Text input â†’ High-quality audio output using Coqui TTS models
  # Runs completely locally - perfect for privacy-focused applications
  - name: "coqui-voice-chat"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "coqui"  # Local Coqui TTS (free, offline)

    # Capabilities: Voice output with local TTS
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "session_id"
        - "return_audio"
        - "tts_voice"      # Speaker name for multi-speaker models
        - "tts_language"   # Language for multilingual models

    config:
      # Coqui TTS settings (local, no API key needed)
      tts_model: "tts_models/en/ljspeech/tacotron2-DDC"  # Default English model
      vocoder_model: "vocoder_models/en/ljspeech/hifigan_v2"  # High-quality vocoder
      tts_format: "wav"
      tts_speed: 1.0  # Speech speed multiplier

      # Popular model alternatives (uncomment to use):
      # Multi-speaker English: "tts_models/en/vctk/vits"
      # Multilingual with voice cloning: "tts_models/multilingual/multi-dataset/xtts_v2"
      # Fast synthesis: "tts_models/en/ljspeech/glow-tts"

      # Audio output
      audio_input_enabled: false  # Text input only
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Fully local voice chat - Whisper STT + Coqui TTS (100% free, no API costs!)
  # Complete voice-to-voice conversation using only open-source models
  # Perfect for: privacy, offline use, cost optimization, self-hosted deployments
  - name: "local-voice-chat-coqui-whisper"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides - ALL LOCAL
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "coqui"  # Coqui TTS for speech synthesis (could be switched to whisper for STT if needed)
    # Note: For STT, you would need to handle Whisper separately or use a multimodal approach
    # This adapter focuses on TTS with Coqui. For full STT+TTS, consider using the multimodal adapter

    # Capabilities: Fully local voice assistant
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "session_id"
        - "return_audio"
        - "tts_voice"
        - "tts_language"

    config:
      # Coqui TTS configuration
      tts_model: "tts_models/en/vctk/vits"  # Multi-speaker model for variety
      tts_speaker: null  # Can specify speaker for multi-speaker models
      tts_language: "en"
      tts_format: "wav"
      tts_speed: 1.0

      # Device selection (auto uses GPU if available)
      device: "auto"  # Options: auto, cpu, cuda

      # Audio I/O
      audio_input_enabled: false  # Set to true if implementing Whisper STT
      audio_output_enabled: true
      return_text: true
      return_audio: true

      # Performance notes:
      # - Tacotron2 + HiFiGAN: High quality, moderate speed
      # - VITS: Fast synthesis, built-in vocoder, multi-speaker support
      # - XTTS v2: Best for voice cloning and multilingual (99 languages)
      # - GlowTTS: Fastest synthesis, good for real-time applications
      #
      # GPU acceleration:
      # - CUDA-enabled GPU provides 5-10x faster synthesis
      # - CPU mode works on any machine but slower
      #
      # Zero cost operation:
      # - No API calls or subscriptions
      # - Models downloaded once, cached locally
      # - Perfect for high-volume applications

  # Hybrid local-cloud voice chat - Whisper STT (local) + Coqui TTS (local) + Cloud LLM
  # Best of both worlds: free audio processing + powerful cloud inference
  - name: "hybrid-voice-chat-local-audio"
    enabled: false  # Enable when you want local audio processing with cloud LLM
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"  # Use cloud LLM for best quality
    model: "gpt-oss:120b"
    audio_provider: "coqui"  # Local Coqui TTS (free)
    # For STT, could integrate Whisper in a preprocessing step

    # Capabilities
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "session_id"
        - "return_audio"

    config:
      # Coqui TTS - optimized for quality
      tts_model: "tts_models/multilingual/multi-dataset/xtts_v2"  # Best quality, multilingual
      tts_language: "en"
      tts_format: "wav"
      tts_speed: 1.0
      device: "cuda"  # Use GPU for best performance

      # Audio output
      audio_output_enabled: true
      return_text: true
      return_audio: true

      # Cost optimization:
      # - STT: Free (local Whisper)
      # - TTS: Free (local Coqui)
      # - LLM: Pay only for cloud inference
      # - Typical savings: 70-90% vs full cloud pipeline

  # Real-time voice chat - WebSocket-based bidirectional audio streaming
  # Enables phone call-like conversations with AI using mixed audio providers
  # Best for: Interactive voice assistants, customer service bots, voice-controlled apps
  - name: "real-time-voice-chat"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "gemini"
    model: "gemini-3-pro-preview"

    # Audio providers - separate STT and TTS for flexibility
    stt_provider: "whisper"      # OpenAI Whisper for speech-to-text (reliable transcription)
    audio_provider: "elevenlabs"  # ElevenLabs or vLLM
    tts_provider: "elevenlabs"  # Explicit TTS provider (same as audio_provider)
    # Alternative STT providers: "whisper" (local, free), "google", "gemini"

    # Capabilities: Real-time WebSocket audio streaming
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      supports_interruption: true       # NEW: Supports user interruptions
      supports_realtime_audio: true     # NEW: Supports WebSocket audio streaming
      optional_parameters:
        - "session_id"
        - "user_id"

    config:
      # WebSocket settings
      websocket_enabled: true
      audio_chunk_size_ms: 100          # Audio chunk size in milliseconds
      silence_threshold: 0.01           # VAD silence threshold
      min_speech_duration_ms: 300       # Minimum speech duration to process
      max_speech_duration_ms: 10000     # Maximum speech duration before forced processing
      silence_duration_ms: 500          # Silence duration to trigger speech end detection

      # Interruption settings
      enable_interruption: false
      interruption_timeout_ms: 500      # Time to wait for interruption

      # Audio settings (uses adapter's stt_provider and audio_provider)
      audio_input_enabled: true
      audio_output_enabled: true
      stt_language: "en"                # Language for speech-to-text (Gemini)
      tts_voice: "pNInz6obpgDQGcFmaJgB"
      # Popular ElevenLabs voices: pNInz6obpgDQGcFmaJgB (Adam), EXAVITQu4vr4xnSDxMaL (Bella),
      #                           21m00Tcm4TlvDq8ikWAM (Rachel), AZnzlk1XvdvUeBnXmlld (Domi)
      tts_format: "mp3_44100_128"       # ElevenLabs format (was "wav" for Gemini)
      return_text: false                # WebSocket mode - audio only
      return_audio: true

      # Performance settings
      max_concurrent_audio_tasks: 4     # Parallel TTS generation
      audio_timeout_seconds: 45         # Timeout for audio generation

      # Connection settings
      max_connection_duration_seconds: 3600  # 1 hour max session
      ping_interval_seconds: 30         # WebSocket keepalive ping interval
