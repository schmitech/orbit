# Ollama Presets Configuration
# Define named presets for different Ollama configurations (CPU/GPU, different models)
# Reference these presets from inference.yaml using: use_preset: "preset-name"

ollama_presets:
  # ═══════════════════════════════════════════════════════════════════════════
  # Granite 4.0-1B on CPU - Optimized for Mac with 16GB RAM
  # ═══════════════════════════════════════════════════════════════════════════
  granite-cpu:
    base_url: "http://localhost:11434"
    model: "granite4:1b"
    stream: true
    think: false  # Enable/disable think mode (shows reasoning in <think> tags)
    # Generation parameters - Optimized for Granite 4.0-1B on CPU Mac with 16GB RAM
    temperature: 0.7  # Balanced for creative yet factual outputs with 1B model
    top_p: 0.9  # Good diversity while maintaining coherence
    top_k: 30  # Optimal token selection for 1B model (lower than larger models)
    min_p: 0.0
    typical_p: 0.9  # Adjusted for 1B model quality
    num_predict: 4096  # Sufficient for detailed responses while managing memory on 16GB RAM
    repeat_penalty: 1.15  # Moderate penalty to prevent repetition while maintaining natural flow
    repeat_last_n: 64  # Check fewer tokens for repetition (appropriate for smaller context window)
    presence_penalty: 0.0
    frequency_penalty: 0.0
    # Mirostat sampling (0=disabled, 1=Mirostat, 2=Mirostat v2)
    mirostat: 0
    mirostat_tau: 4.0  # Adjusted for 1B model
    mirostat_eta: 0.1
    # Context and memory - Optimized for Granite 4.0-1B on CPU Mac with 16GB RAM
    # 1B model is memory-efficient, but 16GB RAM limits context window size
    # Using 8K context provides good balance between functionality and memory usage
    num_ctx: 8192  # 8K context - optimal for 16GB RAM on CPU (1B model uses ~2-3GB base, leaving room for context)
    num_keep: 1024  # Keep reasonable chat history and prompt context for 1B model on CPU
    penalize_newline: false
    # Stop sequences
    stop: []
    # Threading and performance - Optimized for Granite 4.0-1B on CPU Mac
    # Mac CPUs typically have 8-10 cores, using 6 threads for optimal CPU utilization
    num_threads: 6  # Optimal for CPU inference on Mac (balances performance and system responsiveness)
    num_batch: 64  # Smaller batch size for CPU (reduces memory pressure on 16GB RAM)
    # CPU settings - Optimized for Mac CPU (no GPU acceleration)
    num_gpu: 0  # CPU only (0 = no GPU layers, all computation on CPU)
    main_gpu: 0  # Not applicable for CPU but kept for compatibility
    low_vram: false  # Not applicable for CPU mode
    # Memory management - Optimized for CPU with 16GB RAM
    use_mmap: true  # Memory mapping reduces RAM usage (important for 16GB systems)
    use_mlock: false  # Keep false for CPU to allow OS memory management
    vocab_only: false
    numa: false
    # Random seed for reproducible outputs
    seed: null
    # Retry configuration for handling cold starts
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000  # Start with 2 seconds
      max_wait_ms: 30000     # Max 30 seconds per retry
      exponential_base: 2    # Exponential backoff multiplier
    # Timeout configuration
    timeout:
      connect: 10000         # 10 seconds for connection
      total: 120000          # 2 minutes total timeout
      warmup: 120000         # 2 minutes for initial warmup (increased for cold starts)
    # Keep alive - how long to keep model loaded in memory after requests
    # Options: "5m", "10m", "1h", "-1" (indefinite), "0" (unload immediately)
    # Longer values reduce cold start latency but use more VRAM
    keep_alive: "10m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Granite 4.0-1B on GPU - Optimized for systems with GPU/Metal acceleration
  # ═══════════════════════════════════════════════════════════════════════════
  granite-gpu:
    base_url: "http://localhost:11434"
    model: "granite4:1b"
    stream: true
    think: false  # Enable/disable think mode (shows reasoning in <think> tags)
    # Generation parameters - Can be more aggressive with GPU acceleration
    temperature: 0.7
    top_p: 0.9
    top_k: 40  # Slightly higher for GPU (faster processing)
    min_p: 0.0
    typical_p: 0.9
    num_predict: 8192  # Larger output with GPU acceleration
    repeat_penalty: 1.15
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    # Mirostat sampling (0=disabled, 1=Mirostat, 2=Mirostat v2)
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context and memory - Larger context with GPU
    num_ctx: 16384  # 16K context with GPU acceleration
    num_keep: 2048  # Keep more context with GPU
    penalize_newline: false
    # Stop sequences
    stop: []
    # Threading - fewer CPU threads needed when using GPU
    num_threads: 4  # Reduced CPU threads when GPU does heavy lifting
    num_batch: 512  # Larger batch size for GPU (faster processing)
    # GPU settings - Full GPU acceleration
    num_gpu: -1  # -1 = all layers on GPU (maximum acceleration)
    main_gpu: 0  # Primary GPU device
    low_vram: false  # Set to true if running low on VRAM
    # Memory management
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    # Random seed for reproducible outputs
    seed: null
    # Retry configuration
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    # Timeout configuration
    timeout:
      connect: 10000
      total: 120000
      warmup: 120000
    keep_alive: "10m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Qwen3 4B on GPU - Larger model for more capable responses
  # ═══════════════════════════════════════════════════════════════════════════
  qwen3-4b-gpu:
    base_url: "http://localhost:11434"
    model: "qwen3:4b"
    stream: true
    think: true  # Qwen3 supports think mode well
    # Generation parameters
    temperature: 0.6
    top_p: 0.85
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 8192
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Qwen3 supports larger context
    num_ctx: 32768  # 32K context
    num_keep: 4096
    penalize_newline: false
    stop: []
    # GPU optimized settings
    num_threads: 4
    num_batch: 512
    num_gpu: -1  # All layers on GPU
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 180000  # 3 minutes for larger model
      warmup: 180000
    keep_alive: "15m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Qwen3 4B on GPU - Capable inference model with GPU acceleration
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/qwen3
  # 
  # Qwen3 4B is a capable language model for general inference tasks.
  # Good balance of performance and resource usage with GPU acceleration.
  # ═══════════════════════════════════════════════════════════════════════════
  qwen3:4b-gpu:
    base_url: "http://localhost:11434"
    model: "qwen3:4b"
    stream: true
    think: true  # Qwen3 supports think mode well
    # Generation parameters
    temperature: 0.6
    top_p: 0.85
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 8192
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Qwen3 supports larger context
    num_ctx: 32768  # 32K context
    num_keep: 4096
    penalize_newline: false
    stop: []
    # GPU optimized settings
    num_threads: 4  # Reduced CPU threads when GPU does heavy lifting
    num_batch: 512  # Larger batch for GPU efficiency
    num_gpu: -1  # All layers on GPU
    main_gpu: 0
    low_vram: false  # Set to true if running low on VRAM (4B needs ~5-6GB VRAM)
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 180000  # 3 minutes for 4B model
      warmup: 180000
    keep_alive: "15m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Llama 3.2 3B on CPU - Good balance of capability and CPU performance
  # ═══════════════════════════════════════════════════════════════════════════
  llama3-3b-cpu:
    base_url: "http://localhost:11434"
    model: "llama3.2:3b"
    stream: true
    think: false
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 4096
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    num_ctx: 8192
    num_keep: 1024
    penalize_newline: false
    stop: []
    # CPU optimized settings
    num_threads: 6
    num_batch: 64
    num_gpu: 0  # CPU only
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000
      warmup: 120000
    keep_alive: "10m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Gemma 3 4B on CPU - Excellent quality-to-size ratio for edge deployment
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/gemma3
  # 
  # Google's Gemma 3 4B is highly optimized for edge inference.
  # Exceptional performance for its size, very memory efficient (~3GB).
  # Great for general-purpose tasks, reasoning, and instruction following.
  # ═══════════════════════════════════════════════════════════════════════════
  gemma3-4b-cpu:
    base_url: "http://localhost:11434"
    model: "gemma3:4b"
    stream: true
    think: false
    # Generation parameters - Gemma 3 works well with moderate temperatures
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 4096
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Gemma 3 supports up to 128K but limit for CPU/16GB RAM
    num_ctx: 8192  # 8K context for 16GB RAM efficiency
    num_keep: 1024
    penalize_newline: false
    stop: []
    # CPU optimized settings
    num_threads: 6
    num_batch: 64
    num_gpu: 0  # CPU only
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000
      warmup: 120000
    keep_alive: "10m"

  # ═══════════════════════════════════════════════════════════════════════════
  # SmolLM2 1.7B on CPU - Ultra-fast inference for latency-sensitive apps
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/smollm2
  # 
  # HuggingFace's SmolLM2 is designed for maximum speed on edge devices.
  # Only ~1.2GB with excellent instruction-following capabilities.
  # Ideal for: chatbots, quick responses, resource-constrained environments.
  # ═══════════════════════════════════════════════════════════════════════════
  smollm2-1.7b-cpu:
    base_url: "http://localhost:11434"
    model: "SmolLM2:latest"
    stream: true
    think: false
    # Generation parameters - SmolLM2 benefits from slightly lower temps
    temperature: 0.6
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 2048  # Smaller output for speed
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Keep moderate for speed
    num_ctx: 8192
    num_keep: 1024
    penalize_newline: false
    stop: []
    # CPU optimized - can use more threads since model is tiny
    num_threads: 8
    num_batch: 128  # Larger batch for tiny model
    num_gpu: 0  # CPU only
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000   # 2 minutes - allow time for cold starts
      warmup: 120000  # 2 minutes for initial model loading
    keep_alive: "15m"

  # ═══════════════════════════════════════════════════════════════════════════
  # SmolLM2 1.7B on GPU - Ultra-fast inference with NVIDIA CUDA acceleration
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/smollm2
  #
  # HuggingFace's SmolLM2 with full GPU acceleration for VMs with NVIDIA GPUs.
  # Only ~1.8GB VRAM required - fits easily on most GPUs.
  # Ideal for: high-throughput chatbots, low-latency responses, cloud VMs.
  # ═══════════════════════════════════════════════════════════════════════════
  smollm2-1.7b-gpu:
    base_url: "http://localhost:11434"
    model: "SmolLM2:latest"
    stream: true
    think: false
    # Generation parameters - SmolLM2 benefits from slightly lower temps
    temperature: 0.6
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 4096  # Larger output with GPU acceleration
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Larger context with GPU acceleration
    num_ctx: 16384  # 16K context with GPU
    num_keep: 2048  # Keep more context with GPU
    penalize_newline: false
    stop: []
    # GPU optimized - fewer CPU threads when GPU does heavy lifting
    num_threads: 4
    num_batch: 512  # Larger batch size for GPU efficiency
    # GPU settings - Full NVIDIA CUDA acceleration
    num_gpu: -1  # -1 = all layers on GPU (maximum acceleration)
    main_gpu: 0  # Primary GPU device (change for multi-GPU setups)
    low_vram: false  # Set to true if running low on VRAM (unlikely for 1.7B model)
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000   # 2 minutes - allow time for cold starts
      warmup: 120000  # 2 minutes for initial model loading
    keep_alive: "15m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Qwen2.5 3B on CPU - Strong multilingual with good reasoning
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/qwen2.5
  # 
  # Alibaba's Qwen2.5 3B offers excellent quality for its size.
  # Strong multilingual support and coding capabilities.
  # Good balance of speed and capability for 16GB systems.
  # ═══════════════════════════════════════════════════════════════════════════
  qwen2.5-3b-cpu:
    base_url: "http://localhost:11434"
    model: "qwen2.5:3b"
    stream: true
    think: false
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 4096
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    # Context - Qwen2.5 supports up to 128K, limit for 16GB RAM
    num_ctx: 8192
    num_keep: 1024
    penalize_newline: false
    stop: []
    # CPU optimized settings
    num_threads: 6
    num_batch: 64
    num_gpu: 0  # CPU only
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000
      warmup: 120000
    keep_alive: "10m"

  # ═══════════════════════════════════════════════════════════════════════════
  # Phi-4 Mini on CPU - Microsoft's strong reasoning model
  # ═══════════════════════════════════════════════════════════════════════════
  # Model: https://ollama.com/library/phi4-mini
  # 
  # Microsoft's Phi-4 Mini excels at reasoning and structured tasks.
  # ~2.5GB with surprisingly strong performance on complex queries.
  # Great for: code, math, logical reasoning, structured output.
  # ═══════════════════════════════════════════════════════════════════════════
  phi4-mini-cpu:
    base_url: "http://localhost:11434"
    model: "phi4-mini"
    stream: true
    think: false
    # Phi models work well with lower temperatures for reasoning
    temperature: 0.5
    top_p: 0.9
    top_k: 40
    min_p: 0.0
    typical_p: 0.9
    num_predict: 4096
    repeat_penalty: 1.1
    repeat_last_n: 64
    presence_penalty: 0.0
    frequency_penalty: 0.0
    mirostat: 0
    mirostat_tau: 4.0
    mirostat_eta: 0.1
    num_ctx: 8192
    num_keep: 1024
    penalize_newline: false
    stop: []
    # CPU optimized settings
    num_threads: 6
    num_batch: 64
    num_gpu: 0  # CPU only
    main_gpu: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    vocab_only: false
    numa: false
    seed: null
    retry:
      enabled: true
      max_retries: 5
      initial_wait_ms: 2000
      max_wait_ms: 30000
      exponential_base: 2
    timeout:
      connect: 10000
      total: 120000
      warmup: 120000
    keep_alive: "10m"

