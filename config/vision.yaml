# Global vision configuration
vision:
  provider: "gemini"  # Default vision provider: openai, gemini, anthropic
  enabled: true        # Whether vision processing is enabled globally

# Provider-specific configurations
visions:
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}
    api_base: "https://api.openai.com/v1"
    model: "gpt-5"
    temperature: 0.0
    max_tokens: 1000
    stream: false
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds (increased for slow networks)
      total: 90000     # 90 seconds (increased to handle concurrent API calls + retries)
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  gemini:
    enabled: true
    api_key: ${GOOGLE_API_KEY}
    model: "gemini-2.5-flash"
    temperature: 0.0
    top_p: 0.8
    top_k: 20
    max_tokens: 1000
    stream: false
    transport: "rest"  # Use REST to avoid gRPC/ALTS warnings
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds (increased for slow networks)
      total: 90000     # 90 seconds (increased to handle concurrent API calls + retries)
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  anthropic:
    enabled: true
    api_key: ${ANTHROPIC_API_KEY}
    api_base: "https://api.anthropic.com/v1"
    model: "claude-3-5-sonnet-20241022"
    temperature: 0.0
    max_tokens: 1000
    stream: false
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds (increased for slow networks)
      total: 90000     # 90 seconds (increased to handle concurrent API calls + retries)
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  ollama:
    enabled: false
    base_url: "http://localhost:11434"
    model: "qwen3-vl:8b"
    temperature: 0.0
    max_tokens: 1000
    stream: false
    think: false
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds (increased for slow networks)
      total: 90000     # 90 seconds (increased to handle concurrent API calls + retries)
      warmup: 60000    # 1 minute for initial warmup
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  vllm:
    enabled: false
    host: "localhost"
    port: 8000
    model: "llava-hf/llava-1.5-7b-hf"  # Or other multimodal model
    temperature: 0.0
    max_tokens: 1000
    stream: false
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds
      total: 90000     # 90 seconds
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  llama_cpp:
    enabled: false
    # Mode: "direct" (load model locally with Python bindings) or "api" (connect to llama.cpp server)
    mode: "direct"

    # Direct mode configuration
    model_path: "models/llava-v1.5-7b-Q4_K_M.gguf"  # Or other multimodal GGUF model
    chat_format: "llava-1-5"  # Chat format for vision model

    # API mode configuration
    base_url: "http://localhost:8080"  # llama.cpp server URL (OpenAI-compatible API)
    model: "llava"  # Model name to request from the server (for API mode)
    api_key: null  # Optional API key for secured servers

    # Generation parameters
    temperature: 0.0
    max_tokens: 1000
    stream: false

    # Context and threading (direct mode only)
    n_ctx: 4096
    n_threads: 8

    # GPU settings (direct mode only)
    n_gpu_layers: 0  # For GPU/Metal support
    main_gpu: 0
    tensor_split: null

    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds
      total: 90000     # 90 seconds
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

  cohere:
    enabled: true
    api_key: ${COHERE_API_KEY}
    api_base: "https://api.cohere.ai/v2"  # Must use v2 API for vision/multimodal support
    model: "c4ai-aya-vision-32b"  # Cohere's multimodal model
    temperature: 0.0
    max_tokens: 1000
    top_p: 1.0
    stream: false
    # Timeout configuration
    timeout:
      connect: 15000   # 15 seconds (increased for slow networks)
      total: 90000     # 90 seconds (increased to handle concurrent API calls + retries)
    # Retry configuration
    retry:
      enabled: true
      max_retries: 3
      initial_wait_ms: 1000
      max_wait_ms: 30000
      exponential_base: 2

