# Adapter configurations for ORBIT
# This file contains all adapter definitions and can be imported by config.yaml
#
# Provider Overrides:
# Each adapter can optionally override the global providers:
#   - inference_provider: Override which LLM to use (e.g., "ollama", "anthropic", "openai")
#   - embedding_provider: Override which embedding service to use (for retriever adapters)
#   - reranker_provider: Override which reranker to use
#   - model: Override specific model within a provider
#
# Capability-Based Architecture:
# Adapters support explicit capability declarations for cleaner, more maintainable code.
# Capabilities control retrieval behavior, formatting, and parameter handling without hardcoded checks.
#
# Available Capabilities:
#   retrieval_behavior: "none" | "always" | "conditional"
#     - Controls when context retrieval occurs
#     - "none": No retrieval (pure passthrough)
#     - "always": Always retrieve context (QA, Intent, File adapters)
#     - "conditional": Retrieve based on conditions (e.g., only when file_ids present)
#
#   formatting_style: "standard" | "clean" | "custom"
#     - Controls how retrieved documents are formatted
#     - "standard": Include citations and confidence scores (e.g., [1], [2])
#     - "clean": No citations or metadata (prevents LLM from adding citation markers)
#     - "custom": Use custom formatting function
#
#   supports_file_ids: true | false
#     - Whether adapter accepts file_ids parameter for filtering by specific files
#     - Used by file-based and multimodal adapters
#
#   supports_session_tracking: true | false
#     - Whether adapter needs session_id parameter passed to retriever
#     - Used for: analytics, telemetry, session-level logging, conversation tracking
#     - Most adapters don't need this (stateless queries)
#     - Useful for observability and multi-turn conversation analytics
#
#   requires_api_key_validation: true | false
#     - Whether adapter needs API key for authentication/authorization
#     - Used to validate file ownership or access permissions
#
#   skip_when_no_files: true | false
#     - For conditional retrieval: skip retrieval when file_ids is empty
#     - Only applicable when retrieval_behavior is "conditional"
#
#   optional_parameters: ["param1", "param2"]
#     - List of additional parameters to pass from context to retriever
#     - Parameters are only passed if present in the request context
#
# Note: Capabilities are OPTIONAL. If not specified, they are automatically inferred from adapter type.

adapters:
  - name: "simple-chat"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"
    # Provider overrides (optional - use when you want different providers than global defaults)
    inference_provider: "ollama_cloud"  # Override default inference provider
    model: "glm-4.6"     # Specific model for this adapter

    # Capabilities: Pure passthrough - no retrieval
    capabilities:
      retrieval_behavior: "none"         # No context retrieval (pure conversational)
      formatting_style: "standard"       # Standard formatting (not used since no retrieval)
      supports_file_ids: false           # Does not support file filtering
      supports_session_tracking: false   # Does not need session tracking
      requires_api_key_validation: false # No API key validation needed

  - name: "simple-chat-with-files"
    enabled: true
    type: "passthrough"  # Passthrough adapter with file retrieval capabilities
    datasource: "none"  # No actual datasource needed (files managed separately)
    adapter: "multimodal"
    implementation: "implementations.passthrough.multimodal.MultimodalImplementation"
    # Provider overrides (optional - use when you want different providers than global defaults)
    inference_provider: "ollama_cloud"    # Override default inference provider
    model: "glm-4.6"            # Specific model for this adapter
    embedding_provider: "openai"          # Embedding provider for file chunk retrieval (matches uploaded files)
    vision_provider: "openai"             # Vision provider for image files: openai, gemini, anthropic (see vision.yaml)

    # Capabilities: Multimodal - conditional file retrieval
    capabilities:
      retrieval_behavior: "conditional"  # Only retrieves when file_ids are present
      formatting_style: "clean"          # Clean format (no citations) to prevent LLM citation markers
      supports_file_ids: true            # Accepts file_ids parameter for filtering
      supports_session_tracking: true    # Uses session_id for tracking
      requires_api_key_validation: true  # Validates file ownership via API key
      skip_when_no_files: true           # Skip retrieval when file_ids is empty
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "session_id"

    config:
      # File adapter configuration (similar to file-document-qa)
      # Storage configuration
      storage_backend: "filesystem"  # Future: "s3", "minio"
      storage_root: "./uploads"
      max_file_size: 52428800  # 50MB
      
      # Processing configuration
      # Chunking strategy options: "fixed", "semantic", "token", "recursive"
      # These settings override global defaults in config.yaml
      chunking_strategy: "recursive"  # Options: "fixed", "semantic", "token", "recursive"
      chunk_size: 1000  # Characters for fixed/semantic, tokens for token/recursive
      chunk_overlap: 200  # Characters for fixed/semantic, tokens for token/recursive
      
      # Vector store integration
      vector_store: "chroma"  # References stores.yaml
      collection_prefix: "files_"
      
      # Q&A settings
      confidence_threshold: 0.3
      max_results: 5
      return_results: 3

  # Enhanced multimodal adapter with audio file transcription support
  - name: "simple-chat-with-files-audio"
    enabled: true
    type: "passthrough"  # Passthrough adapter with file retrieval and audio transcription
    datasource: "none"
    adapter: "multimodal"
    implementation: "implementations.passthrough.multimodal.MultimodalImplementation"
    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    embedding_provider: "openai"
    vision_provider: "gemini"             # For image files
    audio_provider: "openai"              # For audio file transcription (NEW)

    # Capabilities: Multimodal with audio support
    capabilities:
      retrieval_behavior: "conditional"
      formatting_style: "clean"
      supports_file_ids: true
      supports_session_tracking: true
      requires_api_key_validation: true
      skip_when_no_files: true
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "session_id"

    config:
      # Storage configuration
      storage_backend: "filesystem"
      storage_root: "./uploads"
      max_file_size: 104857600  # 100MB (increased for audio files)
      
      # Audio processing settings
      enable_audio_transcription: true
      audio_transcription_language: null  # Auto-detect language
      
      # Processing configuration
      chunking_strategy: "recursive"
      chunk_size: 1000
      chunk_overlap: 200
      
      # Vector store integration
      vector_store: "chroma"
      collection_prefix: "files_"
      
      # Supported file types (including audio)
      supported_types:
        - "application/pdf"
        - "text/plain"
        - "text/markdown"
        - "text/csv"
        - "application/json"
        - "text/html"
        - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        - "image/png"
        - "image/jpeg"
        - "image/tiff"
        - "audio/wav"
        - "audio/mpeg"
        - "audio/mp3"
        - "audio/ogg"
        - "audio/flac"
        - "audio/webm"
      
      # Q&A settings
      confidence_threshold: 0.3
      max_results: 5
      return_results: 3

  # Voice chat adapter - accepts audio input and returns audio responses
  - name: "voice-chat"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"
    
    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "openai"  # For both STT and TTS
    
    # Capabilities: Voice-enabled chat
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "audio_input"      # Base64-encoded audio data
        - "audio_format"    # Input audio format (mp3, wav, etc.)
        - "language"        # Language code for STT
        - "session_id"
        - "return_audio"     # Whether to return audio response
        - "tts_voice"        # Voice for TTS (alloy, echo, fable, etc.)
    
    config:
      # Audio processing settings
      audio_input_enabled: true
      audio_output_enabled: true
      stt_language: "en-US"     # Default language for speech-to-text
      tts_voice: "alloy"        # Default voice for text-to-speech
      tts_format: "mp3"         # Default output format
      return_text: true         # Also return text alongside audio
      return_audio: true        # Return audio response

  # Audio transcription adapter - specialized for transcribing audio files
  - name: "audio-transcription"
    enabled: true
    type: "retriever"
    datasource: "none"
    adapter: "file"
    implementation: "retrievers.implementations.file.file_retriever.FileVectorRetriever"
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    
    # Provider overrides
    audio_provider: "openai"    # Use OpenAI Whisper for transcription
    embedding_provider: "openai"
    
    # Capabilities: Audio file transcription
    capabilities:
      retrieval_behavior: "conditional"
      formatting_style: "clean"
      supports_file_ids: true
      supports_session_tracking: false
      requires_api_key_validation: true
      skip_when_no_files: true
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "transcription_language"  # Optional: specify language
    
    config:
      # Storage configuration
      storage_backend: "filesystem"
      storage_root: "./uploads"
      max_file_size: 104857600  # 100MB for audio files
      
      # Audio-specific settings
      enable_audio_transcription: true
      transcription_language: null  # Auto-detect, or "en-US", "fr-FR", etc.
      transcription_format: "text"   # Return format: "text", "json", "srt", "vtt"
      
      # Supported audio formats
      supported_types:
        - "audio/wav"
        - "audio/mpeg"
        - "audio/mp3"
        - "audio/mp4"
        - "audio/ogg"
        - "audio/flac"
        - "audio/webm"
      
      # Vector store for transcribed text
      vector_store: "chroma"
      collection_prefix: "audio_transcriptions_"
      
      chunking_strategy: "recursive"
      chunk_size: 2000  # Larger chunks for transcriptions
      chunk_overlap: 200
      
      confidence_threshold: 0.3
      max_results: 10
      return_results: 5

  # Multilingual voice assistant - supports translation and voice I/O
  - name: "multilingual-voice-assistant"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"
    
    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "openai"  # Google supports many languages
    
    # Capabilities: Multilingual voice assistant
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "audio_input"
        - "source_language"    # Language of input audio
        - "target_language"    # Language for output audio
        - "session_id"
    
    config:
      # Language settings
      default_source_language: "auto"  # Auto-detect input language
      default_target_language: "en-US" # Default output language
      
      # Google TTS settings (multilingual)
      tts_model: "neural2"
      tts_voice: "en-US-Neural2-A"
      tts_language_code: "en-US"
      tts_audio_encoding: "MP3"
      
      # Google STT settings
      stt_model: "latest_long"
      stt_language_code: "en-US"
      stt_sample_rate: 16000
      stt_encoding: "LINEAR16"
      
      # Audio I/O
      audio_input_enabled: true
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Premium voice chat with ElevenLabs (high-quality TTS)
  - name: "premium-voice-chat"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "elevenlabs"  # High-quality TTS
    # Note: ElevenLabs is TTS-only, STT would need separate provider
    # For now, this adapter expects text input (can be extended)

    # Capabilities: Premium voice output
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "voice_id"           # ElevenLabs voice ID
        - "session_id"
        - "return_audio"       # Return audio response

    config:
      # ElevenLabs TTS settings
      tts_model: "eleven_multilingual_v2"
      tts_voice: "EXAVITQu4vr4xnSDxMaL"  # Default voice (Sarah)
      tts_format: "mp3_44100_128"
      tts_stability: 0.5
      tts_similarity_boost: 0.75
      tts_style: 0.0
      tts_use_speaker_boost: true

      # Audio output
      audio_output_enabled: true
      return_text: true
      return_audio: true

  # Local audio file transcription with Whisper (no API costs!)
  # Upload audio files → Get transcriptions and chat about the content
  # Uses local Whisper for transcription - completely free and offline!
  - name: "local-voice-chat"
    enabled: true
    type: "passthrough"
    datasource: "none"
    adapter: "multimodal"
    implementation: "implementations.passthrough.multimodal.MultimodalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    embedding_provider: "openai"
    audio_provider: "whisper"  # Local Whisper for audio transcription (free, offline)

    # Capabilities: Audio file transcription with chat
    capabilities:
      retrieval_behavior: "conditional"  # Only retrieves when audio files are uploaded
      formatting_style: "clean"          # Clean format (no citations)
      supports_file_ids: true            # Accepts audio file uploads
      supports_session_tracking: true
      requires_api_key_validation: true  # Validates file ownership
      skip_when_no_files: true           # Skip retrieval when no files
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "session_id"

    config:
      # Storage configuration
      storage_backend: "filesystem"
      storage_root: "./uploads"
      max_file_size: 104857600  # 100MB for audio files

      # Audio transcription settings
      enable_audio_transcription: true
      audio_transcription_language: null  # Auto-detect language

      # Whisper-specific settings (nested under 'sounds' for WhisperAudioService)
      sounds:
        whisper:
          enabled: true
          model_size: "base"       # Whisper model: tiny, base, small, medium, large-v3
          device: "cpu"            # Device: auto, cpu, cuda
          language: null           # Auto-detect language (99 languages supported)
          task: "transcribe"       # Task: transcribe or translate

      # Processing configuration
      chunking_strategy: "recursive"
      chunk_size: 1000
      chunk_overlap: 200

      # Vector store integration
      vector_store: "chroma"
      collection_prefix: "audio_files_"

      # Supported audio file types (transcribed with Whisper)
      supported_types:
        - "audio/wav"
        - "audio/mpeg"
        - "audio/mp3"
        - "audio/mp4"
        - "audio/ogg"
        - "audio/flac"
        - "audio/webm"
        - "audio/x-m4a"
        - "audio/aac"

      # Q&A settings
      confidence_threshold: 0.3
      max_results: 5
      return_results: 3

      # Audio output: DISABLED (Whisper doesn't support TTS)
      return_audio: false

      # Performance notes:
      # - tiny: ~32x faster, good for quick testing
      # - base: ~16x faster, good balance (RECOMMENDED)
      # - small: ~6x faster, better accuracy
      # - medium: ~2x faster, high quality
      # - large-v3: Best accuracy, slower

  # Hybrid local voice chat - Whisper STT + vLLM TTS (fully local, no API costs!)
  - name: "vllm-voice-chat-hybrid"
    enabled: false
    type: "passthrough"
    datasource: "none"
    adapter: "conversational"
    implementation: "implementations.passthrough.conversational.ConversationalImplementation"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    audio_provider: "vllm"  # vLLM for TTS (Orpheus model) - local, free
    # NOTE: For STT, client should send text or use separate Whisper processing
    # This adapter focuses on TTS output using local vLLM server

    # Capabilities: Voice output with local TTS
    capabilities:
      retrieval_behavior: "none"
      formatting_style: "standard"
      supports_file_ids: false
      supports_session_tracking: true
      requires_api_key_validation: false
      optional_parameters:
        - "session_id"
        - "return_audio"
        - "tts_voice"

    config:
      # vLLM TTS settings (local, no API key needed)
      tts_model: "canopylabs/orpheus-3b-0.1-ft"
      tts_voice: "tara"  # Options: tara, leah, jess, leo, dan, mia, zac, zoe
      tts_format: "mp3"

      # Audio output
      audio_input_enabled: false  # Use Whisper separately for STT
      audio_output_enabled: true
      return_text: true
      return_audio: true
  
  # This is an example of a QA adapter for a SQL database.
  # It is used as an example of how to create a QA adapter for a SQL database.
  # Run /examples/sample-db-setup.sh sqlite to create a sample database and api key before using this adapter.
  - name: "qa-sql"
    enabled: true
    type: "retriever"
    datasource: "sqlite"
    adapter: "qa"
    implementation: "retrievers.implementations.qa.QASSQLRetriever"

    # Provider overrides (optional - use when you want different providers than global defaults)
    inference_provider: "cohere"    # Override default inference provider
    model: "command-r7b-12-2024"            # Specific model for this adapter
    reranker_provider: "cohere"     # Override default reranker provider (improves accuracy)
    reranker_model: "rerank-english-v3.0"     # Specific model for this adapter

    # Capabilities: Standard QA retriever
    capabilities:
      retrieval_behavior: "always"       # Always retrieves context from database
      formatting_style: "standard"       # Standard format with citations and confidence scores
      supports_file_ids: false           # Does not support file filtering
      supports_session_tracking: false   # Does not need session tracking
      requires_api_key_validation: false # API key used for access control, not ownership validation
      optional_parameters:
        - "api_key"

    config:
      # QA-specific settings
      confidence_threshold: 0.3
      max_results: 5         # Retrieve up to 5 documents
      return_results: 3      # Return top 3 after retrieval
      reranker_top_n: 5      # Optional: limit reranked results (overrides return_results)
      
      # Unified retriever features (available for all SQL adapters)
      use_connection_pool: false  # SQLite doesn't support traditional pooling (file-based)
      pool_size: 1  # Not applicable for SQLite
      connection_timeout: 30  # Connection timeout in seconds
      
      # Query monitoring (automatically provided by unified base)
      enable_query_monitoring: true
      query_timeout: 5000  # 5 seconds
      
      # Security and access control (recommended)
      table: "city"  # Specify the exact table for single-table access
      allowed_columns: ["id", "question", "answer", "category", "confidence"]  # Limit accessible columns
      security_filter: "active = 1"  # Only return active Q&A pairs
      
      # Performance optimization
      cache_ttl: 1800  # Cache results for 30 minutes
      
    # Fault tolerance settings for this adapter. This overrides the default settings in config.yaml
    # Use this as example for other adapters if you need to override the default settings.
    fault_tolerance:
      operation_timeout: 15.0          # Lower timeout for local database operations
      failure_threshold: 10            # Higher threshold for local operations (more reliable)
      recovery_timeout: 30.0           # Short base timeout for local DB
      success_threshold: 5             # Multiple successes to close circuit
      max_recovery_timeout: 120.0      # Max 2 minutes for local DB
      enable_exponential_backoff: true # Enable backoff for local DB
      enable_thread_isolation: false   # No isolation needed for local SQLite operations
      enable_process_isolation: false  # SQLite is lightweight, no process isolation needed
      max_retries: 3                   # Retry failed queries
      retry_delay: 0.5                 # Short delay between retries for local DB
      cleanup_interval: 3600.0         # Clean up stats every hour
      retention_period: 86400.0        # Keep stats for 24 hours
      event_handler:
        type: "default"                # Use default filesystem logger
      
  # This is an example of a QA adapter for a ChromaDB database. (Disabled by default)
  # It is used as an example of how to create a QA adapter for a ChromaDB database.
  - name: "qa-vector-chroma"
    enabled: true
    type: "retriever"
    datasource: "chroma"
    adapter: "qa"
    implementation: "retrievers.implementations.qa.QAChromaRetriever"

    # Provider overrides - this example shows using different providers
    inference_provider: "openai"
    embedding_provider: "openai"
    
    config:
      collection: "city"
      confidence_threshold: 0.3
      distance_scaling_factor: 2.0
      max_results: 5         # Retrieve 5 documents from vector store
      return_results: 3      # Return top 3 to LLM
      # reranker_top_n: 3    # Optional: rerank and keep top 3 (improves quality)
      timezone: "America/Toronto"

  - name: "qa-vector-qdrant-demo"
    enabled: false
    type: "retriever"
    datasource: "qdrant"
    adapter: "qa"
    implementation: "retrievers.implementations.qa.QAQdrantRetriever"

    # Provider overrides
    inference_provider: "ollama"      # Use Ollama for inference
    embedding_provider: "openai"      # Use Ollama for embeddings
    reranker_provider: "ollama"       # Use Ollama for reranking

    config:
      collection: "demo"
      confidence_threshold: 0.3
      score_scaling_factor: 1.0
      max_results: 5         # Retrieve 5 documents from Qdrant
      return_results: 3      # Return top 3 to LLM
      reranker_top_n: 3    # Optional: rerank top 3 for best quality
      timezone: "America/Toronto"

  - name: "intent-sql-sqlite-contact"
    enabled: true
    type: "retriever"
    datasource: "sqlite"
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentSQLiteRetriever"
    database: "utils/sql-intent-template/examples/sqlite/contact/contact.db"
    # Provider overrides - intent adapters benefit from powerful models
    inference_provider: "ollama_cloud"    # Cloud LLM for complex intent classification
    model: "gpt-oss:20b"           # Large model for better intent understanding
    embedding_provider: "cohere"          # Local embeddings for template matching
    # reranker_provider: "ollama"           # Optional: Jina AI reranker for template relevance

    config:
      # SQLite database path is configured in datasources.yaml
      check_same_thread: false  # Allow multi-threaded access

      # Path to the domain definition file
      domain_config_path: "utils/sql-intent-template/examples/sqlite/contact/contact-domain.yaml"
      template_library_path:
        - "utils/sql-intent-template/examples/sqlite/contact/contact-templates.yaml"

      # Name for the vector store collection for templates
      template_collection_name: "contact_intent_templates"
      # Store configuration - references stores.yaml
      store_name: "chroma"
      confidence_threshold: 0.4

      # Maximum templates to consider
      max_templates: 5

      # Return top N results
      return_results: 10

      # Template loading settings
      reload_templates_on_start: true  # Whether to reload templates on startup
      force_reload_templates: true  # Force reload even if templates exist in storage

      # Unified retriever features
      use_connection_pool: false  # SQLite doesn't support traditional pooling (file-based)
      pool_size: 1  # Not applicable for SQLite
      connection_timeout: 30  # Connection timeout in seconds

      # Query monitoring (automatically provided by unified base)
      enable_query_monitoring: true
      query_timeout: 5000  # 5 seconds

  - name: "intent-sql-sqlite-classified"
    enabled: true
    type: "retriever"
    datasource: "sqlite"
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentSQLiteRetriever"
    database: "../demos/classified-data/classified_data.db"
    inference_provider: "ollama_cloud"
    model: "minimax-m2:cloud"
    embedding_provider: "cohere"
    reranker_provider: "cohere"
    config:
      reranker_top_n: 10      # Optional: limit reranked results (overrides return_results)
      check_same_thread: false  # Allow multi-threaded access

      # Path to the domain definition file
      domain_config_path: "utils/sql-intent-template/examples/sqlite/classified-data/classified_data_domain.yaml"
      template_library_path:
        - "utils/sql-intent-template/examples/sqlite/classified-data/classified_data_templates.yaml"

      # Name for the vector store collection for templates
      template_collection_name: "classified_data_intent_templates"
      # Store configuration - references stores.yaml
      store_name: "chroma"
      confidence_threshold: 0.4

      # Maximum templates to consider
      max_templates: 5

      # Return top N results
      return_results: 100

      # Template loading settings
      reload_templates_on_start: false  # Whether to reload templates on startup
      force_reload_templates: false  # Force reload even if templates exist in storage

      # Unified retriever features
      use_connection_pool: false  # SQLite doesn't support traditional pooling (file-based)
      pool_size: 1  # Not applicable for SQLite
      connection_timeout: 30  # Connection timeout in seconds

      # Query monitoring (automatically provided by unified base)
      enable_query_monitoring: true
      query_timeout: 5000  # 5 seconds
  
  - name: "intent-duckdb-analytics"
    enabled: true
    type: "retriever"
    datasource: "duckdb"
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentDuckDBRetriever"
    database: "utils/duckdb-intent-template/examples/analytics/analytics.duckdb"
    inference_provider: "ollama_cloud"
    model: "glm-4.6"
    embedding_provider: "openai"
    config:
      # Path to the domain definition file
      domain_config_path: "utils/duckdb-intent-template/examples/analytics/analytics_domain.yaml"
      template_library_path:
        - "utils/duckdb-intent-template/examples/analytics/analytics_templates.yaml"

      # Name for the vector store collection for templates
      template_collection_name: "duckdb_analytics_templates"
      # Store configuration - references stores.yaml
      store_name: "chroma"
      confidence_threshold: 0.4

      # Maximum templates to consider
      max_templates: 5

      # Return top N results
      return_results: 100

      # Template loading settings
      reload_templates_on_start: true  # Whether to reload templates on startup
      force_reload_templates: true  # Force reload even if templates exist in storage

      # DuckDB-specific configuration
      # Database path is configured in datasources.yaml (defaults to examples/duckdb/analytics.duckdb)
      # Can override here if needed:
      # database: "path/to/custom.duckdb"  # Optional: override datasource default
      read_only: true  # Allow concurrent reads from multiple processes (no writes needed for intent queries)
      access_mode: "READ_ONLY"  # Multiple processes can read simultaneously (see https://duckdb.org/docs/stable/connect/concurrency)
      threads: null  # Use default thread count

      # Query monitoring (automatically provided by unified base)
      enable_query_monitoring: true
      query_timeout: 5000  # 5 seconds

  - name: "intent-sql-postgres"
    enabled: true
    type: "retriever"
    datasource: "postgres"
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentPostgreSQLRetriever"
    inference_provider: "ollama_cloud"
    model: "gpt-oss:120b"
    embedding_provider: "cohere"
    # reranker_provider: "ollama"

    # Capabilities: Standard intent retriever
    # Note: Intent adapters use the same capabilities as QA adapters (always retrieve, standard formatting)
    capabilities:
      retrieval_behavior: "always"       # Always retrieves matching intents/templates
      formatting_style: "standard"       # Standard format with citations and confidence scores
      supports_file_ids: false           # Does not support file filtering
      supports_session_tracking: false   # Does not need session tracking
      requires_api_key_validation: false # API key used for access control only
      optional_parameters:
        - "api_key"

    config:
      # Path to the domain definition file
      domain_config_path: "utils/sql-intent-template/examples/postgres/customer-orders/customer_order_domain.yaml"
      template_library_path:
        - "utils/sql-intent-template/examples/postgres/customer-orders/customer_orders_templates.yaml"
        - "utils/sql-intent-template/examples/postgres/customer-orders/business_intelligence_templates.yaml"
        - "utils/sql-intent-template/examples/postgres/customer-orders/advanced_analytics_templates.yaml"
        - "utils/sql-intent-template/examples/postgres/customer-orders/comparative_analysis_templates.yaml"
      # Name for the vector store collection for templates
      # reranker_top_n: 40      # Optional: limit reranked results (overrides return_results)
      template_collection_name: "intent_query_templates"
      # Store configuration - references stores.yaml
      store_name: "chroma"
      confidence_threshold: 0.4
      # Maximum templates to consider
      max_templates: 5
      # Return top N results
      return_results: 100

      # Template loading settings
      reload_templates_on_start: true  # Whether to reload templates on startup
      force_reload_templates: true  # Force reload even if templates exist in storage

      # Unified retriever features
      use_connection_pool: true  # Enable connection pooling
      pool_size: 5  # Connection pool size
      connection_timeout: 30  # Connection timeout in seconds

      # Query monitoring (automatically provided by new base)
      enable_query_monitoring: true
      query_timeout: 5000  # 5 seconds

  # Elasticsearch Intent Adapter - Application Logs
  # This adapter demonstrates Elasticsearch Query DSL generation from natural language
  # Connection parameters are inherited from config/datasources.yaml
  - name: "intent-elasticsearch-app-logs"
    enabled: true
    type: "retriever"
    datasource: "elasticsearch"  # References datasources.yaml elasticsearch config
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentElasticsearchRetriever"
    inference_provider: "ollama_cloud"
    model: "minimax-m2:cloud"
    embedding_provider: "openai"
    # reranker_provider: "ollama"
    config:
      # Domain and template configuration
      domain_config_path: "utils/elasticsearch-intent-template/examples/application-logs/templates/logs_domain.yaml"
      template_library_path:
        - "utils/elasticsearch-intent-template/examples/application-logs/templates/logs_templates.yaml"

      # reranker_top_n: 3      # Optional: limit reranked results (overrides return_results)
      # Vector store configuration for template matching
      template_collection_name: "elasticsearch_logs_templates"
      store_name: "chroma"  # References stores.yaml

      # Intent matching configuration
      confidence_threshold: 0.4
      max_templates: 5
      return_results: 10

      # Template loading settings
      reload_templates_on_start: true
      force_reload_templates: true

      # Elasticsearch-specific configuration
      # Note: Connection parameters (node, auth, verify_certs) are in datasources.yaml
      # Only adapter-specific settings go here
      index_pattern: "application-logs-demo"  # Default index pattern for queries
      use_query_dsl: true  # Enable Query DSL processing
      enable_aggregations: false  # Enable aggregation support
      enable_highlighting: false  # Enable search result highlighting
      default_size: 100  # Default number of results to return

    # Example fault tolerance settings (optional)
    fault_tolerance:
      operation_timeout: 30.0
      failure_threshold: 5
      recovery_timeout: 60.0
      max_retries: 3
      retry_delay: 1.0

  # HTTP JSON API Adapter - JSONPlaceholder (Simple Testing API)
  # This adapter uses a fake REST API for testing and debugging HTTP adapter functionality
  # No authentication required - perfect for isolating parameter extraction issues
  - name: "intent-http-jsonplaceholder"
    enabled: false
    type: "retriever"
    datasource: "http"  # Uses HTTP datasource placeholder
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentHTTPJSONRetriever"
    inference_provider: "ollama"
    model: "gpt-oss:20b"
    embedding_provider: "openai"
    config:
      # Domain and template configuration
      domain_config_path: "utils/http-intent-template/examples/jsonplaceholder/templates/jsonplaceholder_domain.yaml"
      template_library_path:
        - "utils/http-intent-template/examples/jsonplaceholder/templates/jsonplaceholder_templates.yaml"

      # Vector store configuration for template matching
      template_collection_name: "jsonplaceholder_http_templates"
      store_name: "chroma"  # References stores.yaml

      # Intent matching configuration
      confidence_threshold: 0.4
      max_templates: 5
      return_results: 10
      verbose: false  # Enable debug logging

      # Template loading settings
      reload_templates_on_start: true
      force_reload_templates: true

      # HTTP-specific configuration
      base_url: "https://jsonplaceholder.typicode.com"
      default_timeout: 30
      enable_retries: true
      max_retries: 3
      retry_delay: 1.0

      # No authentication required for JSONPlaceholder

    # Fault tolerance settings
    fault_tolerance:
      operation_timeout: 30.0
      failure_threshold: 5
      recovery_timeout: 60.0
      max_retries: 3
      retry_delay: 1.0

  # Firecrawl Intent Adapter - Web Scraping with Intelligent Chunking
  # This adapter demonstrates web scraping using Firecrawl with natural language queries
  # Supports both cloud API (api.firecrawl.dev) and self-hosted Firecrawl deployments
  # Features intelligent content chunking for large documents (e.g., Wikipedia articles)
  - name: "intent-firecrawl-webscrape"
    enabled: true
    type: "retriever"
    datasource: "http"  # Reuse HTTP placeholder
    adapter: "intent"
    implementation: "retrievers.implementations.intent.IntentFirecrawlRetriever"
    inference_provider: "ollama_cloud"
    model: "gpt-oss:20b"
    embedding_provider: "openai"  # Used for chunk ranking
    config:
      # TESTING: Using minimal templates for validation
      # Switch to firecrawl_domain.yaml and firecrawl_templates.yaml for full Wikipedia templates
      domain_config_path: "utils/firecrawl-intent-template/examples/web-scraping/templates/firecrawl_domain.yaml"
      template_library_path:
        - "utils/firecrawl-intent-template/examples/web-scraping/templates/firecrawl_templates.yaml"

      # Vector store for template matching
      template_collection_name: "firecrawl_test_templates"  # Using test collection name
      store_name: "chroma"

      # Intent matching (lower threshold for testing)
      confidence_threshold: 0.3
      max_templates: 5
      return_results: 1

      # Template loading
      reload_templates_on_start: true
      force_reload_templates: true

      # Enable verbose logging for debugging
      verbose: true

      # Firecrawl configuration
      # For cloud API: set base_url to https://api.firecrawl.dev/v1
      # For self-hosted: set to your instance URL
      base_url: "https://api.firecrawl.dev/v1"
      default_timeout: 60  # Increased for large pages
      default_formats: ["markdown"]

      # Authentication (for cloud API)
      auth:
        type: "bearer_token"
        token_env: "FIRECRAWL_API_KEY"
        header_name: "Authorization"
        token_prefix: "Bearer"

      # ========================================
      # Content Chunking Configuration
      # ========================================
      # Automatically chunks large web content to prevent exceeding LLM context limits
      # and improve response relevance through semantic search

      enable_chunking: true                    # Enable/disable chunking
      max_chunk_tokens: 4000                   # Maximum tokens per chunk (~16KB)
      chunk_overlap_tokens: 200                # Overlap between chunks for context continuity
      min_chunk_tokens: 500                    # Minimum chunk size

      # Embedding model limits (CRITICAL: Set based on your embedding provider)
      # OpenAI text-embedding-3-*: 8191 tokens → Use 7500 for safety
      # OpenAI text-embedding-ada-002: 8191 tokens → Use 7500 for safety
      # Cohere embed-english-v3.0: 512 tokens → Use 450 for safety!
      # Jina jina-embeddings-v3: 8192 tokens → Use 7500 for safety
      max_embedding_tokens: 7500               # Max tokens with safety buffer (accounts for estimation error)

      # Chunk storage and retrieval
      chunks_collection: "firecrawl_chunks"    # Vector store collection for chunks
      chunk_cache_ttl_hours: 24                # Cache duration (24 hours for Wikipedia)

      # Chunk ranking and selection
      top_chunks_to_return: 3                  # Number of most relevant chunks to return
      min_chunk_similarity: 0.3                # Minimum similarity score for chunk retrieval

      # Performance benefits:
      # - 75-90% reduction in context size
      # - Faster response times (cached chunks retrieved in <10ms)
      # - Better relevance (only return sections related to query)
      # - Cost optimization (fewer tokens processed by LLM)
      # - Automatic chunk splitting if content exceeds embedding limits
      # ========================================

    fault_tolerance:
      operation_timeout: 60.0  # Increased for large content
      failure_threshold: 5
      recovery_timeout: 60.0
      max_retries: 2
      retry_delay: 2.0

  # MongoDB Intent Retriever for Sample MFlix Database
  - name: "intent-mongodb-mflix"
    enabled: true
    type: "retriever"
    datasource: "mongodb"
    adapter: "intent"
    implementation: "retrievers.implementations.intent.intent_mongodb_retriever.IntentMongoDBRetriever"
    inference_provider: "ollama_cloud"
    model: "minimax-m2:cloud"
    embedding_provider: "openai"
    reranker_provider: "anthropic"
    config:
      reranker_top_n: 10
      # Domain and template configuration
      domain_config_path: "utils/mongodb-intent-template/examples/sample_mflix/templates/mflix_domain.yaml"
      template_library_path:
        - "utils/mongodb-intent-template/examples/sample_mflix/templates/mflix_templates.yaml"
        - "utils/mongodb-intent-template/examples/sample_mflix/templates/mflix_advanced_templates.yaml"

      # Vector store configuration for template matching
      template_collection_name: "intent_mongodb_templates"
      store_name: "chroma"  # References stores.yaml

      # Intent matching configuration
      confidence_threshold: 0.4
      max_templates: 5
      return_results: 50

      # Template loading settings
      reload_templates_on_start: true
      force_reload_templates: true

      # MongoDB-specific configuration
      # Note: Connection parameters (connection_string, host, port, auth) are in datasources.yaml
      # Only adapter-specific settings go here
      database: "sample_mflix"  # Database name (can override datasource default)
      default_collection: "movies"  # Default collection for queries
      default_limit: 100  # Default number of results to return
      max_limit: 1000  # Maximum allowed limit
      enable_text_search: true  # Enable text search with regex
      case_insensitive_regex: true  # Case-insensitive regex by default

      # Placeholder base_url to satisfy parent class
      base_url: "http://localhost:27017"

    # Fault tolerance settings
    fault_tolerance:
      operation_timeout: 15.0
      failure_threshold: 10
      recovery_timeout: 30.0
      success_threshold: 5
      max_recovery_timeout: 120.0
      enable_exponential_backoff: true
      enable_thread_isolation: false
      enable_process_isolation: false
      max_retries: 3
      retry_delay: 1.0
      cleanup_interval: 3600.0
      retention_period: 86400.0
      event_handler:
        type: "default"

  # File Adapter - Document Q&A
  # Supports uploading and querying files (PDF, DOCX, CSV, TXT, HTML, JSON)
  # Uses dual-path strategy: vector stores for documents, DuckDB for structured data
  - name: "file-document-qa"
    enabled: true
    type: "retriever"
    datasource: "none"  # File adapter manages its own storage, no external datasource needed
    adapter: "file"
    implementation: "retrievers.implementations.file.file_retriever.FileVectorRetriever"

    # Provider overrides
    inference_provider: "ollama_cloud"
    model: "minimax-m2:cloud"
    embedding_provider: "openai"
    vision_provider: "gemini"  # Vision provider for image files: openai, gemini, anthropic (see vision.yaml)
    audio_provider: "openai"   # Audio provider for audio file transcription: openai, google, ollama (see sound.yaml)

    # Capabilities: File retriever with clean formatting
    capabilities:
      retrieval_behavior: "always"       # Always retrieves from uploaded files
      formatting_style: "clean"          # Clean format (no citations) to prevent LLM citation markers
      supports_file_ids: true            # Accepts file_ids parameter to filter by specific files
      supports_session_tracking: false   # Does not need session tracking
      requires_api_key_validation: true  # Validates file ownership via API key
      optional_parameters:
        - "file_ids"
        - "api_key"

    config:
      # Storage configuration
      storage_backend: "filesystem"  # Future: "s3", "minio"
      storage_root: "./uploads"
      max_file_size: 104857600  # 100MB (increased for audio files)
      
      # Processing configuration
      # Chunking strategy options: "fixed", "semantic", "token", "recursive"
      # These settings override global defaults in config.yaml
      chunking_strategy: "semantic"  # Options: "fixed", "semantic", "token", "recursive"
      chunk_size: 1000  # Characters for fixed/semantic, tokens for token/recursive
      chunk_overlap: 200  # Characters for fixed/semantic, tokens for token/recursive
      
      # Optional: Tokenizer configuration (inherits from global config if not specified)
      # tokenizer: "gpt2"  # Options: "character" (default), "gpt2", "tiktoken", etc.
      # use_tokens: false  # For fixed strategy: use token-based instead of character-based
      
      # Optional: Strategy-specific chunking options (inherits from global config if not specified)
      # chunking_options:
      #   model_name: "all-MiniLM-L6-v2"  # For semantic chunking
      #   use_advanced: false  # Enable advanced semantic chunking
      #   min_characters_per_chunk: 24  # For recursive chunking
      
      # Vector store integration
      vector_store: "chroma"  # References stores.yaml
      collection_prefix: "files_"
      
      # Supported file types
      supported_types:
        - "application/pdf"
        - "text/plain"
        - "text/markdown"
        - "text/csv"
        - "application/json"
        - "text/html"
        - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        - "application/vnd.openxmlformats-officedocument.presentationml.presentation"  # PPTX (via Docling)
        - "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"  # XLSX (via Docling)
        # Code file types
        - "text/x-python"
        - "text/x-python-script"  # Alternative MIME type for Python files
        - "text/x-java-source"
        - "text/x-java"
        - "text/x-sql"
        - "text/javascript"
        - "application/javascript"
        - "text/typescript"
        - "application/typescript"
        - "text/x-c++src"
        - "text/x-csrc"
        - "text/x-c"
        - "text/x-go"
        - "text/x-rust"
        - "text/x-ruby"
        - "text/x-php"
        - "text/x-shellscript"
        - "text/x-sh"
        - "text/yaml"
        - "text/x-yaml"
        - "text/xml"
        - "application/xml"
        - "text/css"
        - "text/x-scss"
        - "text/x-sass"
        - "text/x-less"
        - "image/png"
        - "image/jpeg"
        - "image/tiff"
        # Audio file types (transcribed to text using audio_provider)
        - "audio/wav"
        - "audio/mpeg"
        - "audio/mp3"
        - "audio/mp4"
        - "audio/ogg"
        - "audio/flac"
        - "audio/webm"
        - "audio/x-m4a"
        - "audio/aac"
        - "text/vtt"
      
      # Audio processing settings
      enable_audio_transcription: true  # Enable audio file transcription
      audio_transcription_language: null  # Auto-detect language, or specify "en-US", "fr-FR", etc.
      
      # Q&A settings
      confidence_threshold: 0.3
      max_results: 5
      return_results: 3
      
      # DuckDB integration for structured data (CSV, Parquet)
      enable_duckdb_path: false
      duckdb_store: "duckdb"