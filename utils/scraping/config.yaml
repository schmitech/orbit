ollama:
    # Ollama server URL and model
    base_url: "http://3.96.184.5:11434"
    model: "gpt-oss:20b"

    # Q/A extraction guidance:
    # - Keep generations direct and avoid meta phrases by using conservative sampling.
    # - Consider the suggested ranges in comments; keep your current values if they work for you.

    # Lower temperature reduces boilerplate and meta language (recommended ~0.1)
    temperature: 0.1

    # Tighter nucleus sampling keeps outputs focused (suggest 0.70–0.75 for stricter control)
    top_p: 0.8   # e.g., 0.75 for stricter

    # Limit candidate tokens to more likely choices (suggest 30–40 for Q/A)
    top_k: 20    # e.g., 40 for stricter

    # Discourage repeated phrasing like "as described in the text"
    repeat_penalty: 1.1  # e.g., 1.15 for stronger penalty

    # Generation length. For Q generation 256–384 is often enough; answers may need more.
    num_predict: 1024 # e.g., 384 for Qs, 512 for As

    # Context window. 8k–16k is usually sufficient; larger uses more memory.
    num_ctx: 32000

    # Runtime controls
    num_threads: 8
    keep_alive: "30m"   # keep model warm between requests
    timeout: 300        # request timeout in seconds

    # Optional (not wired yet): deterministic runs
    # seed: 42

    # Example QA-tuned alternative
    # temperature: 0.1
    # top_p: 0.75
    # top_k: 40
    # repeat_penalty: 1.15
    # num_predict: 384
    # num_ctx: 16000
