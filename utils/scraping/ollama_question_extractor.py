"""
Ollama Docling Question Extractor (Improved Version)

This script extracts questions and answers from markdown files generated by Docling using an Ollama instance.
This improved version includes optimized paraphrasing logic with batch processing, parallel execution,
and better quality control.

Key Improvements:
-----------------
1. Batch paraphrase generation - Process multiple questions in a single API call
2. Parallel processing - Generate paraphrases concurrently for different questions
3. Smarter retry logic - Only regenerate missing paraphrases
4. Enhanced prompt engineering - More specific guidance for variation types
5. Separate paraphrase caching - Reusable paraphrase cache
6. Semantic validation - Basic similarity checking for quality control
7. Structured output parsing - More reliable JSON extraction

Overview:
---------
The script processes markdown files from a specified directory or single file (typically output from docling-crawler.py),
extracting relevant questions that can be answered from the content, and then generates answers to those questions.
It creates a JSON file containing structured question-answer pairs, which can be used for:
- Creating FAQs 
- Building Q&A datasets
- Training chatbots or Q&A systems
- Knowledge base construction

Features:
---------
1. Optimized for Docling markdown output format
2. Single file or bulk directory processing with concurrent execution
3. Caching of questions, answers, and paraphrases to reduce API calls
4. Command-line arguments for flexibility
5. Progress reporting and real-time output writing
6. Intelligent prompt design to ensure high-quality questions and answers
7. HTTP connection pooling and keep-alive for better performance
8. Model warm-up to reduce cold start latency
9. Granular error handling with smart retry strategies
10. Configurable timeouts and connection optimization
11. Comprehensive error handling for Ollama connection failures
12. Model existence validation and health checks
13. Detailed troubleshooting guidance for common issues
14. Batch paraphrase generation for improved efficiency
15. Parallel paraphrase processing with asyncio
16. Semantic validation of generated paraphrases

Requirements:
------------
- Python 3.7+
- A running Ollama instance
- A `config.yaml` file with Ollama parameters
- Required packages: aiohttp, pyyaml, requests

Usage:
------
Note: Run docling-crawler.py first to generate the markdown files from documents.

Basic usage (directory):
python ollama_question_extractor.py --input ./csed-md-files --output ./questions.json

Process a single markdown file:
python ollama_question_extractor.py --mdfile ./document.md --output ./questions.json

With rate limiting and optimization:
python ollama_question_extractor.py --input ./csed-md-files --output ./questions.json --delay 0.5 --concurrent 3

Process in batches (useful for large datasets):
python ollama_question_extractor.py --input ./csed-md-files --output ./questions.json --batch-size 10

With custom timeout and no warm-up:
python ollama_question_extractor.py --input ./csed-md-files --output ./questions.json --timeout 600 --no-warmup

Test Ollama connection without processing files:
python ollama_question_extractor.py --test-connection

With enhanced paraphrasing options:
python ollama_question_extractor.py --input ./csed-md-files --output ./questions.json --group-questions --paraphrases 3 --paraphrase-batch-size 5

Command-line Arguments:
---------------------
--input, -i     : Input directory containing markdown files (default: ./data/docs)
--mdfile        : Process a single markdown file instead of a directory
--output, -o    : Output JSON file path (default: ./data/questions.json)
--quiet, -q     : Run quietly with minimal output
--no-cache      : Skip cache and regenerate all questions and answers
--delay         : Delay in seconds between API calls (default: 0)
--concurrent    : Maximum concurrent API requests (default: 5)
--max-qa        : Maximum Q&A pairs per file (default: 300)
--batch-size    : Process files in batches of this size (0=all at once)
--no-warmup     : Skip model warm-up (default: False)
--timeout       : Request timeout in seconds (default: 300)
--test-connection : Test Ollama connection and model availability without processing files
--group-questions : Group paraphrased questions per answer (many-to-one output)
--paraphrases   : Number of paraphrased variants per question (default: 2)
--paraphrase-batch-size : Number of questions to paraphrase in a single API call (default: 5)
--parallel-paraphrases : Enable parallel paraphrase generation (default: True)
--validate-paraphrases : Enable semantic validation of paraphrases (default: True)

Input Modes:
-----------
1. Directory Mode (--input): Process all markdown files in a directory
2. Single File Mode (--mdfile): Process one specific markdown file

Output Format:
-------------
Default mode (one-to-one):
- Each item is an object: { "question": string, "answer": string }

Grouped mode (many-to-one, enable with --group-questions):
- Each item is an object: { "questions": string[], "answer": string }
  - "questions" contains the original question plus useful paraphrases/synonyms (e.g., "Who is the CEO?")

Note: The answers are self-contained and include relevant contact information, 
URLs, and references directly embedded within the text for better RAG compatibility.

Pipeline Integration:
--------------------
This is step 3 of the QA generation pipeline:
1. url-extractor.py or firecrawl-url-extractor.py -> Extract URLs from website
2. docling-crawler.py -> Convert pages to markdown
3. ollama_question_extractor.py -> Generate QA pairs (YOU ARE HERE)

Use pipeline_orchestrator.py to run all steps automatically.
"""

import json
import argparse
import os
import asyncio
import re
import time
import sys
from pathlib import Path
import yaml
import aiohttp
import requests
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict
import hashlib

# Load Ollama configuration from config.yaml
try:
    with open('config.yaml', 'r') as f:
        config = yaml.safe_load(f)
        ollama_config = config.get('ollama', {})
except FileNotFoundError:
    raise ValueError("config.yaml not found. Please create it with your Ollama settings.")

# Ollama settings
OLLAMA_BASE_URL = ollama_config.get('base_url')
OLLAMA_MODEL = ollama_config.get('model')
OLLAMA_TEMPERATURE = ollama_config.get('temperature', 0.1)
OLLAMA_TOP_P = ollama_config.get('top_p', 0.8)
OLLAMA_TOP_K = ollama_config.get('top_k', 20)
OLLAMA_REPEAT_PENALTY = ollama_config.get('repeat_penalty', 1.1)
OLLAMA_NUM_PREDICT = ollama_config.get('num_predict', 1024)
OLLAMA_NUM_CTX = ollama_config.get('num_ctx', 32000)
OLLAMA_NUM_THREADS = ollama_config.get('num_threads', 8)
OLLAMA_TIMEOUT = ollama_config.get('timeout', 300)  # Request timeout in seconds
OLLAMA_KEEP_ALIVE = ollama_config.get('keep_alive', '30m')  # Keep model loaded

# Global session for connection pooling
session = None

# Feature toggles (overridden by CLI args)
GROUP_QUESTIONS = False
PARAPHRASE_COUNT = 2
PARAPHRASE_BATCH_SIZE = 5
PARALLEL_PARAPHRASES = True
VALIDATE_PARAPHRASES = True
DEBUG_PARAPHRASES = False

# Paraphrase cache
paraphrase_cache = {}

if not OLLAMA_BASE_URL or not OLLAMA_MODEL:
    raise ValueError("Ollama base_url or model not found in config.yaml.")

print(f"Ollama configured with model: {OLLAMA_MODEL} at {OLLAMA_BASE_URL}")
print(f"Timeout: {OLLAMA_TIMEOUT}s, Keep-alive: {OLLAMA_KEEP_ALIVE}")

# Validate Ollama connection and model existence at startup
def validate_ollama_setup():
    """Validate that Ollama is accessible and the model exists."""
    try:
        # Test basic connection to Ollama server
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code != 200:
            raise ConnectionError(f"Ollama server at {OLLAMA_BASE_URL} is not responding properly (HTTP {response.status_code})")
        
        # Check if the specified model exists
        models_response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if models_response.status_code == 200:
            available_models = models_response.json().get('models', [])
            model_names = [model.get('name', '') for model in available_models]
            
            if OLLAMA_MODEL not in model_names:
                raise ValueError(f"Model '{OLLAMA_MODEL}' not found on Ollama server. Available models: {', '.join(model_names)}")
            
            print(f"✓ Model '{OLLAMA_MODEL}' found and available")
        else:
            raise ConnectionError(f"Failed to retrieve model list from Ollama server (HTTP {models_response.status_code})")
            
    except requests.exceptions.ConnectionError:
        raise ConnectionError(f"Cannot connect to Ollama server at {OLLAMA_BASE_URL}. Please ensure Ollama is running and accessible.")
    except requests.exceptions.Timeout:
        raise ConnectionError(f"Connection to Ollama server at {OLLAMA_BASE_URL} timed out. Please check your network connection.")
    except Exception as e:
        raise RuntimeError(f"Unexpected error validating Ollama setup: {e}")

# Validate Ollama setup at import time (skip if just showing help)
if '--help' not in sys.argv and '-h' not in sys.argv:
    try:
        validate_ollama_setup()
    except Exception as e:
        print(f"❌ Ollama validation failed: {e}")
        raise

def test_ollama_connection():
    """Test function to verify Ollama connection and model availability."""
    try:
        validate_ollama_setup()
        print("✅ Ollama connection test passed!")
        print(f"   Server: {OLLAMA_BASE_URL}")
        print(f"   Model: {OLLAMA_MODEL}")
        return True
    except Exception as e:
        print(f"❌ Ollama connection test failed: {e}")
        return False


# Constants
MAX_QA_PAIRS = int(os.getenv('MAX_QA_PAIRS', '50'))  # Default max per file
MAX_CONCURRENT_REQUESTS = int(os.getenv('MAX_CONCURRENT_REQUESTS', '5'))
API_DELAY = float(os.getenv('API_DELAY', '0'))  # Delay between API calls in seconds
MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))  # Max retries for failed API calls
throttler = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

def parse_docling_markdown(content):
    """Parses Docling-generated markdown content.
    
    Docling produces cleaner markdown with:
    - Title as first heading (# Title)
    - Metadata as bullet points after title
    - Clean body content without navigation elements
    """
    frontmatter = {}
    lines = content.split('\n')
    body_start = 0
    
    # Extract title from first heading
    for i, line in enumerate(lines):
        if line.strip().startswith('# '):
            frontmatter['title'] = line.replace('# ', '').strip()
            body_start = i + 1
            break
    
    # Look for metadata in bullet points after title (author, date, etc.)
    metadata_section = []
    for i in range(body_start, min(body_start + 10, len(lines))):
        if lines[i].strip().startswith('- '):
            metadata_section.append(lines[i].replace('- ', '').strip())
        elif lines[i].strip() and not lines[i].strip().startswith('#'):
            # End of metadata section
            break
        body_start = i + 1
    
    # Parse metadata
    for item in metadata_section:
        # Common patterns in Docling output
        if re.match(r'^[A-Z][a-z]+ \d{1,2}, \d{4}$', item):  # Date pattern
            frontmatter['date'] = item
        elif re.match(r'^[A-Z][a-z]+ [A-Z][a-z]+', item):  # Name pattern
            frontmatter['author'] = item
    
    # Join remaining lines as body
    body = '\n'.join(lines[body_start:])
    
    # Clean Docling-specific artifacts
    body = clean_docling_content(body)
    
    return frontmatter, body.strip()

def clean_docling_content(body):
    """Clean Docling-specific content artifacts."""
    # Remove navigation sections
    body = re.sub(r'## Related posts.*?(?=\n#|\Z)', '', body, flags=re.DOTALL | re.IGNORECASE)
    body = re.sub(r'Read More ›\n*', '', body)
    
    # Remove image placeholders
    body = re.sub(r'<!-- image -->\n*', '', body)
    body = re.sub(r'!\s*\[.*?\]\s*\(.*?\)\n*', '', body)
    
    # Remove footer/contact sections that are repetitive
    body = re.sub(r'^Contact us$', '', body, flags=re.MULTILINE)
    body = re.sub(r'^###### About.*?(?=\n#|\Z)', '', body, flags=re.DOTALL | re.MULTILINE)
    body = re.sub(r'^###### Services.*?(?=\n#|\Z)', '', body, flags=re.DOTALL | re.MULTILINE)
    body = re.sub(r'^## Land Acknowledgement.*?(?=\n#|\Z)', '', body, flags=re.DOTALL | re.MULTILINE)
    
    # Remove duplicate sections (Docling sometimes duplicates menu items)
    seen_lines = set()
    cleaned_lines = []
    for line in body.split('\n'):
        line_stripped = line.strip()
        # Skip duplicate navigation items
        if line_stripped.startswith('- ') and line_stripped in seen_lines:
            continue
        seen_lines.add(line_stripped)
        cleaned_lines.append(line)
    
    body = '\n'.join(cleaned_lines)
    
    # Remove excessive whitespace
    body = re.sub(r'\n{3,}', '\n\n', body)
    
    return body

def load_markdown_files_from_directory(directory):
    """Load all markdown files from a directory, parse Docling format, and clean content."""
    files = []
    directory_path = Path(directory)
    
    if not directory_path.exists():
        print(f"Warning: Directory '{directory}' does not exist.")
        return files
    
    for file_path in directory_path.glob('**/*.md'):
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                frontmatter, cleaned_body = parse_docling_markdown(content)
                
                # Add filename as source if not already present
                if 'source' not in frontmatter:
                    frontmatter['source'] = file_path.stem.replace('-', ' ').title()
                
                if len(cleaned_body.split()) > 20: # Only process files with some content
                    files.append((str(file_path), cleaned_body, frontmatter))
                else:
                    print(f"Skipping {file_path} due to short content after cleaning.")
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
    
    return files

def flatten_nested_lists(nested_lists):
    """Flatten a list of lists."""
    flattened_list = []
    for sublist in nested_lists:
        flattened_list.extend(sublist)
    return flattened_list

def append_results_to_file(qa_pairs, output_filepath, verbose=False):
    """Append Q&A pairs to the output file in real-time."""
    import fcntl  # For file locking
    
    try:
        # Create file if it doesn't exist with initial JSON array
        if not output_filepath.exists():
            with open(output_filepath, 'w', encoding='utf-8') as f:
                json.dump([], f)
        
        # Read existing data
        with open(output_filepath, 'r', encoding='utf-8') as f:
            try:
                existing_data = json.load(f)
            except json.JSONDecodeError:
                existing_data = []
        
        # Append new Q&A pairs
        existing_data.extend(qa_pairs)
        
        # Write back to file with file locking for concurrent access
        with open(output_filepath, 'w', encoding='utf-8') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)  # Exclusive lock
            json.dump(existing_data, f, indent=2, ensure_ascii=False)
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)  # Unlock
        
        if verbose and qa_pairs:
            # Get current file size for monitoring
            file_size = output_filepath.stat().st_size if output_filepath.exists() else 0
            file_size_kb = file_size / 1024
            print(f"    ✓ Saved {len(qa_pairs)} Q&A pair(s) to {output_filepath} (file now: {file_size_kb:.1f}KB)")
            
    except Exception as e:
        print(f"    ⚠ Warning: Could not append to output file: {e}")

async def get_session():
    """Get or create aiohttp session with optimized settings."""
    global session
    if session is None or session.closed:
        # Optimized connector settings
        connector = aiohttp.TCPConnector(
            limit=100,  # Total connection pool size
            limit_per_host=20,  # Connections per host
            keepalive_timeout=60,  # Keep connections alive for 60s
            enable_cleanup_closed=True,
            ttl_dns_cache=300,  # DNS cache for 5 minutes
        )
        
        # Optimized timeout settings
        timeout = aiohttp.ClientTimeout(
            total=OLLAMA_TIMEOUT,  # Total request timeout
            connect=30,  # Connection timeout
            sock_read=OLLAMA_TIMEOUT - 30  # Socket read timeout
        )
        
        session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'Connection': 'keep-alive'}
        )
    
    # Verify connection is still alive
    try:
        async with session.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5) as response:
            if response.status != 200:
                raise ConnectionError(f"Ollama server health check failed (HTTP {response.status})")
    except Exception as e:
        # Close the session and recreate it
        if session and not session.closed:
            await session.close()
        session = None
        raise ConnectionError(f"Ollama server connection lost: {e}")
    
    return session

async def warm_up_model():
    """Warm up the model to avoid cold start latency."""
    try:
        session = await get_session()
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": "Hello",
            "stream": False,
            "keep_alive": OLLAMA_KEEP_ALIVE,
            "options": {"num_predict": 1}
        }
        
        async with session.post(f"{OLLAMA_BASE_URL}/api/generate", json=payload) as response:
            if response.status == 200:
                print(f"✓ Model {OLLAMA_MODEL} warmed up successfully")
            elif response.status == 404:
                raise ValueError(f"Model '{OLLAMA_MODEL}' not found on Ollama server. Please check if the model is properly installed.")
            elif response.status == 500:
                raise RuntimeError(f"Ollama server error during model warm-up. The model '{OLLAMA_MODEL}' may be corrupted or not properly loaded.")
            else:
                error_text = await response.text()
                raise RuntimeError(f"Model warm-up failed with HTTP {response.status}: {error_text}")
    except aiohttp.ClientError as e:
        raise ConnectionError(f"Failed to connect to Ollama server during warm-up: {e}")
    except Exception as e:
        raise RuntimeError(f"Model warm-up failed: {e}")

async def run_model(prompt, retries=0):
    """Run the Ollama model with the given prompt, with retry logic and optimized HTTP."""
    try:
        async with throttler:
            if API_DELAY > 0:
                await asyncio.sleep(API_DELAY)

            payload = {
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "keep_alive": OLLAMA_KEEP_ALIVE,  # Keep model loaded
                "options": {
                    "temperature": OLLAMA_TEMPERATURE,
                    "top_p": OLLAMA_TOP_P,
                    "top_k": OLLAMA_TOP_K,
                    "repeat_penalty": OLLAMA_REPEAT_PENALTY,
                    "num_predict": OLLAMA_NUM_PREDICT,
                    "num_ctx": OLLAMA_NUM_CTX,
                    "num_thread": OLLAMA_NUM_THREADS,
                }
            }
            
            session = await get_session()
            async with session.post(f"{OLLAMA_BASE_URL}/api/generate", json=payload) as response:
                if response.status == 200:
                    response_json = await response.json()
                    if 'response' in response_json:
                        return response_json['response'].strip()
                    else:
                        return str(response_json).strip()
                else:
                    error_text = await response.text()
                    raise aiohttp.ClientResponseError(
                        request_info=response.request_info,
                        history=response.history,
                        status=response.status,
                        message=f"HTTP {response.status}: {error_text}"
                    )

    except asyncio.TimeoutError as e:
        if retries < MAX_RETRIES:
            wait_time = (2 ** retries) * 3  # Longer wait for timeout errors
            print(f"⏱ Timeout error (retry {retries + 1}/{MAX_RETRIES} in {wait_time}s): {e}")
            await asyncio.sleep(wait_time)
            return await run_model(prompt, retries + 1)
        else:
            print(f"⏱ Timeout error after {MAX_RETRIES} retries: {e}")
            return "ERROR"
    except aiohttp.ClientResponseError as e:
        if e.status == 404:
            raise ValueError(f"Model '{OLLAMA_MODEL}' not found on Ollama server. Please check if the model is properly installed.")
        elif e.status == 503:  # Service unavailable - model might be loading
            if retries < MAX_RETRIES:
                wait_time = (2 ** retries) * 5  # Longer wait for model loading
                print(f"🔄 Model loading (retry {retries + 1}/{MAX_RETRIES} in {wait_time}s): HTTP {e.status}")
                await asyncio.sleep(wait_time)
                return await run_model(prompt, retries + 1)
            else:
                raise RuntimeError(f"Model '{OLLAMA_MODEL}' is still loading after {MAX_RETRIES} retries. Please wait and try again.")
        elif e.status == 429:  # Rate limited
            if retries < MAX_RETRIES:
                wait_time = (2 ** retries) * 4
                print(f"🚦 Rate limited (retry {retries + 1}/{MAX_RETRIES} in {wait_time}s): HTTP {e.status}")
                await asyncio.sleep(wait_time)
                return await run_model(prompt, retries + 1)
            else:
                raise RuntimeError(f"Rate limit exceeded after {MAX_RETRIES} retries. Please reduce concurrent requests or increase delay.")
        elif e.status == 500:
            raise RuntimeError(f"Ollama server error (HTTP 500). The model '{OLLAMA_MODEL}' may be corrupted or not properly loaded.")
        else:
            raise RuntimeError(f"HTTP error {e.status}: {e.message}")
    except aiohttp.ClientError as e:
        if retries < MAX_RETRIES:
            wait_time = (2 ** retries) * 2
            print(f"🔌 Connection error (retry {retries + 1}/{MAX_RETRIES} in {wait_time}s): {e}")
            await asyncio.sleep(wait_time)
            return await run_model(prompt, retries + 1)
        else:
            raise ConnectionError(f"Failed to connect to Ollama server after {MAX_RETRIES} retries: {e}")
    except Exception as e:
        raise RuntimeError(f"Unexpected error during model execution: {e}")

def extract_questions_from_output(output):
    """Extract numbered questions from text output."""
    question_pattern = re.compile(r"^\s*\d+\.\s*(.+)$", re.MULTILINE)
    questions = question_pattern.findall(output)
    
    if questions and not re.search(r"[.!?)]$", questions[-1].strip()):
        print(f"WARNING: Removing incomplete question: '{questions[-1]}'")
        questions.pop()
    
    return questions

async def extract_questions_in_chunks(file_path, text, frontmatter, verbose=False):
    """Extract questions from long text by processing it in chunks for better coverage."""
    words = text.split()
    text_length = len(words)
    
    # Split into manageable chunks of ~1500 words with minimal overlap for performance
    chunk_size = 1500
    overlap = 150
    chunks = []
    
    for i in range(0, text_length, chunk_size - overlap):
        chunk_words = words[i:i + chunk_size]
        chunk_text = ' '.join(chunk_words)
        chunks.append(chunk_text)
    
    if verbose:
        print(f"  → Processing long document in {len(chunks)} chunks for comprehensive extraction")
    
    all_questions = []
    questions_per_chunk = max(15, MAX_QA_PAIRS // len(chunks))
    
    for i, chunk in enumerate(chunks):
        if verbose:
            print(f"    → Processing chunk {i+1}/{len(chunks)} ({questions_per_chunk} questions target)")
        
        # Extract questions from this chunk
        chunk_questions = await extract_questions_from_chunk(
            file_path, chunk, frontmatter, questions_per_chunk, verbose
        )
        all_questions.extend(chunk_questions)
    
    # Deduplicate while preserving order
    seen = set()
    unique_questions = []
    for q in all_questions:
        q_lower = q[2].lower()  # The question text is at index 2
        if q_lower not in seen:
            seen.add(q_lower)
            unique_questions.append(q)
    
    if verbose:
        print(f"  → Extracted {len(unique_questions)} unique questions from {len(chunks)} chunks")
    
    # Limit to MAX_QA_PAIRS if we got too many
    if len(unique_questions) > MAX_QA_PAIRS:
        unique_questions = unique_questions[:MAX_QA_PAIRS]
        if verbose:
            print(f"  → Limited to {MAX_QA_PAIRS} questions (max_qa limit)")
    
    return unique_questions

async def extract_questions_from_chunk(file_path, chunk_text, frontmatter, target_questions, verbose=False):
    """Extract questions from a chunk of text."""
    context_prompt = ""
    if frontmatter.get('title'):
        context_prompt += f"Document title: '{frontmatter['title']}'.\n"
    
    extraction_prompt = f"""
    Extract EXACTLY {target_questions} questions from this text section.
    {context_prompt}
    
    REQUIREMENTS:
    1. Generate {target_questions} diverse questions
    2. Cover all key points in this section
    3. Make questions specific and self-contained
    4. Use proper names and terms from the text
    5. NO meta-references like "according to the text"
    
    FORMAT: Number each question:
    1. Question?\n2. Question?\n...
    
    Text section:
    {chunk_text}
    """
    
    output = await run_model(extraction_prompt)
    questions = extract_questions_from_output(output)
    questions = [clean_question_text(q) for q in questions]
    
    return [(file_path, chunk_text, q.strip(), frontmatter) for q in questions if q.strip()]

async def extract_questions_from_text(file_path, text, frontmatter, verbose=False):
    """Extract questions from text using the AI model, with context from frontmatter."""
    text_length = len(text.split())
    
    # For very long documents, process in chunks to get better coverage
    # Temporarily disabled for testing - will process as single document
    # if text_length > 3000:
    #     return await extract_questions_in_chunks(file_path, text, frontmatter, verbose)
    
    
    # Calculate target questions based on text length, but respect MAX_QA_PAIRS limit
    # More aggressive extraction for better coverage
    if text_length < 500:
        base_target = 8
    elif text_length < 1000:
        base_target = 15
    elif text_length < 2000:
        base_target = 25
    elif text_length < 4000:
        base_target = 35
    else:
        base_target = min(50, max(40, text_length // 100))  # ~1 question per 100 words for long docs
    
    # Ensure we don't exceed the global MAX_QA_PAIRS limit
    target_questions = min(base_target, MAX_QA_PAIRS)
    
    if verbose:
        print(f"  → Target questions: {target_questions} (text length: {text_length} words, max_qa: {MAX_QA_PAIRS})")
    
    context_prompt = ""
    if frontmatter.get('title'):
        context_prompt += f"The title of the document is: '{frontmatter['title']}'.\n"
    if frontmatter.get('description'):
        context_prompt += f"The description is: '{frontmatter['description']}'.\n"

    extraction_prompt = f"""
    You are an expert Q&A extractor. Read the following text thoroughly and extract comprehensive questions.
    {context_prompt}
    
    TASK: Extract EXACTLY {target_questions} diverse, high-quality questions from this content.
    
    CRITICAL REQUIREMENTS:
    1. Generate EXACTLY {target_questions} questions - this is mandatory
    2. Cover ALL major topics, sections, and key points in the text
    3. Include diverse question types:
       - Factual (Who, What, When, Where)
       - Definitional (What is...)
       - Procedural (How does...)
       - Comparative (What's the difference...)
       - Analytical (Why is...)
       - List-based (What are the...)
    4. Extract questions about:
       - Key concepts and definitions
       - Important names, dates, numbers, and statistics
       - Processes, procedures, and methodologies  
       - Goals, objectives, and outcomes
       - Features, benefits, and characteristics
       - Challenges, problems, and solutions
       - Relationships and connections
       - Examples and case studies
    5. Make questions specific and self-contained
    6. Use proper names and specific terms from the text
    7. NO meta-references ("according to the text", "the document says")
    8. Ensure even distribution across the entire document
    
    FORMAT: Number each question on a new line:
    1. First question?
    2. Second question?
    ...continue until {target_questions}
    
    Text to process:
    {text}
    """
    
    output = await run_model(extraction_prompt)
    questions = extract_questions_from_output(output)
    # Safety net cleanup to remove meta phrases if model still outputs them
    questions = [clean_question_text(q) for q in questions]
    # Deduplicate while preserving order
    seen = set()
    cleaned = []
    for q in questions:
        qn = q.strip()
        if qn and qn.lower() not in seen:
            seen.add(qn.lower())
            cleaned.append(qn)
    questions = cleaned
    
    result = [(file_path, text, question.strip(), frontmatter) for question in questions]
    
    # If we got fewer questions than target, warn but continue
    if len(result) < target_questions and verbose:
        print(f"  ⚠ Warning: Only extracted {len(result)} questions out of target {target_questions}")
    
    return result

def clean_question_text(q: str) -> str:
    """Remove meta phrases like 'according to the text' from a question and normalize punctuation."""
    original = q or ""
    s = original.strip()
    # Remove leading meta qualifiers
    lead_patterns = [
        r"^\s*(according to|based on|as described in|as mentioned in|from|within|in)\s+the\s+(provided\s+)?(text|document|article|content|passage|material|context)[:,]?\s*",
        r"^\s*(according to|based on)\s+(this|the)\s+(page|section|content)[:,]?\s*",
    ]
    for pat in lead_patterns:
        s = re.sub(pat, "", s, flags=re.IGNORECASE)
    # Remove trailing references to the text
    trail_patterns = [
        r"\s*(in|from|according to|as described in|as mentioned in)\s+the\s+(provided\s+)?(text|document|article|content|passage|material|context)\s*[\.?]*\s*$",
        r"\s*(in|from)\s+(this|the)\s+(page|section|content)\s*[\.?]*\s*$",
    ]
    for pat in trail_patterns:
        s = re.sub(pat, "", s, flags=re.IGNORECASE)
    # Normalize whitespace and punctuation
    s = re.sub(r"\s+", " ", s).strip().strip('"\'')
    if s and not s.endswith('?'):
        s = s.rstrip('.').rstrip('!').rstrip(':').strip()
        s += '?'
    # Capitalize first letter if not already
    if s:
        s = s[0].upper() + s[1:]
    return s

async def generate_answer(question, source, frontmatter):
    """Generate an answer for a question using the given source text and frontmatter."""
    context_prompt = ""
    if frontmatter.get('title'):
        context_prompt += f"The title of the document is: '{frontmatter['title']}'.\n"
    
    # Add source URL if available in frontmatter
    source_url = frontmatter.get('source', '')
    url_context = ""
    if source_url and source_url.startswith('http'):
        url_context = f"This information comes from: {source_url}\n"

    answering_prompt = f"""
    You are an expert answer generator. Provide a comprehensive, detailed answer based on the source text.
    {context_prompt}{url_context}
    
    ANSWER REQUIREMENTS:
    1. Be thorough and comprehensive - include ALL relevant details from the text
    2. Use specific names, numbers, dates, and facts mentioned in the source
    3. Structure complex answers with bullet points or numbered lists when appropriate
    4. Include context and background information when it helps understanding
    5. Incorporate any relevant contact information, URLs, or references naturally
    6. Make the answer self-contained - readable without needing the question
    7. Use natural, professional language - avoid meta phrases like "the text states"
    8. If multiple aspects exist, address all of them
    9. Include specific examples or details that support the main points
    10. For quantitative information, include all numbers, percentages, and measurements mentioned
    
    FORMATTING for complex answers:
    - Use **bold** for key terms or names
    - Use bullet points for lists
    - Use line breaks to separate different aspects
    - Include specific details in parentheses when helpful
    
    Question: {question}
    
    Source text:
    {source}
    """
    
    answer = await run_model(answering_prompt)
    return answer

def _extract_list_items(output: str) -> list:
    """Extract list items from model output (handles JSON array, numbered, or bulleted lists)."""
    if not output:
        return []
    text = output.strip()
    # Try JSON array first
    try:
        data = json.loads(text)
        if isinstance(data, list):
            return [str(x).strip() for x in data if isinstance(x, (str, int, float))]
    except Exception:
        pass
    # Try to extract JSON array from text
    json_match = re.search(r'\[.*?\]', text, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group())
            if isinstance(data, list):
                return [str(x).strip() for x in data if isinstance(x, (str, int, float))]
        except Exception:
            pass
    # Fallback: numbered list like "1. ..." or bullets "- ..."
    items = []
    for line in text.splitlines():
        m = re.match(r"\s*(?:\d+\.|[-*•])\s+(.*)$", line)
        if m:
            items.append(m.group(1).strip())
    # If nothing matched, take sentences split by newline as last resort
    if not items and text:
        items = [s.strip("- *•\t ") for s in text.splitlines() if s.strip()]
    return items

def _normalize_question(q: str) -> str:
    return clean_question_text(q)

def get_paraphrase_cache_key(question: str, answer: str) -> str:
    """Generate a cache key for paraphrase lookups."""
    content = f"{question}||{answer}"
    return hashlib.md5(content.encode()).hexdigest()

def calculate_similarity(q1: str, q2: str) -> float:
    """Calculate basic similarity between two questions (simple Jaccard similarity)."""
    # Simple word-based similarity
    words1 = set(q1.lower().split())
    words2 = set(q2.lower().split())
    if not words1 or not words2:
        return 0.0
    intersection = words1 & words2
    union = words1 | words2
    return len(intersection) / len(union)

def validate_paraphrases(original: str, paraphrases: List[str], min_similarity: float = 0.3, max_similarity: float = 0.9) -> List[str]:
    """Validate that paraphrases are neither too similar nor too different."""
    validated = []
    for para in paraphrases:
        similarity = calculate_similarity(original, para)
        if min_similarity <= similarity <= max_similarity:
            validated.append(para)
        elif DEBUG_PARAPHRASES:
            print(f"    ⚠ Rejected paraphrase (similarity={similarity:.2f}): {para[:50]}...")
    return validated

async def generate_batch_paraphrases(question_answer_pairs: List[Tuple[str, str, dict]], count: int = 2) -> Dict[str, List[str]]:
    """Generate paraphrases for multiple questions in a single API call for efficiency.
    
    Args:
        question_answer_pairs: List of (question, answer, frontmatter) tuples
        count: Number of paraphrases to generate per question
        
    Returns:
        Dictionary mapping original questions to their paraphrases
    """
    if not question_answer_pairs:
        return {}
    
    # Check cache first
    results = {}
    uncached_pairs = []
    
    for question, answer, frontmatter in question_answer_pairs:
        cache_key = get_paraphrase_cache_key(question, answer)
        if cache_key in paraphrase_cache:
            results[question] = paraphrase_cache[cache_key]
            if DEBUG_PARAPHRASES:
                print(f"    ↳ Using cached paraphrases for: {question[:50]}...")
        else:
            uncached_pairs.append((question, answer, frontmatter))
    
    if not uncached_pairs:
        return results
    
    # Prepare batch prompt
    batch_size = len(uncached_pairs)
    questions_text = "\n".join([f"{i+1}. \"{q}\"" for i, (q, _, _) in enumerate(uncached_pairs)])
    answers_text = "\n".join([f"{i+1}. {a[:200]}..." if len(a) > 200 else f"{i+1}. {a}" 
                              for i, (_, a, _) in enumerate(uncached_pairs)])
    
    prompt = f"""
    You are creating paraphrased questions for a Q&A dataset.
    
    I have {batch_size} questions, each with their definitive answer.
    For each question, generate EXACTLY {count} alternative phrasings that would lead to the same answer.
    
    Original Questions:
    {questions_text}
    
    Corresponding Answers (for context):
    {answers_text}
    
    REQUIREMENTS:
    1. Generate EXACTLY {count} paraphrases for EACH of the {batch_size} questions
    2. Use different variation strategies:
       - Syntactic: Change sentence structure while keeping meaning
       - Lexical: Use synonyms and alternative words
       - Perspective: Ask from different viewpoints (e.g., "What is X?" vs "Can you explain X?")
    3. Keep paraphrases concise and natural
    4. Include entity names when relevant
    5. Avoid meta phrases like "according to the text"
    6. Ensure each paraphrase would reasonably lead to the same answer
    
    OUTPUT FORMAT:
    Return a JSON object where each key is the question number (1, 2, 3...) and the value is an array of {count} paraphrased questions.
    Example: {{"1": ["paraphrase1", "paraphrase2"], "2": ["paraphrase1", "paraphrase2"]}}
    
    IMPORTANT: Return ONLY the JSON object, no other text.
    """
    
    output = await run_model(prompt)
    
    if DEBUG_PARAPHRASES:
        print(f"    ↳ Batch paraphrase raw output: {output[:500]}...")
    
    # Parse the batch results
    try:
        # Try to extract JSON from the output
        json_match = re.search(r'\{.*\}', output, re.DOTALL)
        if json_match:
            batch_results = json.loads(json_match.group())
        else:
            batch_results = json.loads(output)
        
        # Map results back to original questions
        for i, (question, answer, frontmatter) in enumerate(uncached_pairs):
            key = str(i + 1)
            if key in batch_results and isinstance(batch_results[key], list):
                paraphrases = [_normalize_question(p) for p in batch_results[key]]
                # Validate if enabled
                if VALIDATE_PARAPHRASES:
                    paraphrases = validate_paraphrases(question, paraphrases)
                # Deduplicate
                seen = set([question.lower()])
                cleaned = []
                for p in paraphrases:
                    if p and p.lower() not in seen:
                        cleaned.append(p)
                        seen.add(p.lower())
                    if len(cleaned) >= count:
                        break
                
                results[question] = cleaned
                # Cache the results
                cache_key = get_paraphrase_cache_key(question, answer)
                paraphrase_cache[cache_key] = cleaned
            else:
                results[question] = []
                
    except (json.JSONDecodeError, KeyError) as e:
        if DEBUG_PARAPHRASES:
            print(f"    ⚠ Failed to parse batch paraphrase output: {e}")
        # Fallback to empty paraphrases for all
        for question, _, _ in uncached_pairs:
            results[question] = []
    
    return results

async def generate_paraphrases_parallel(questions_with_context: List[Tuple[str, str, str, dict]], count: int = 2) -> Dict[str, List[str]]:
    """Generate paraphrases for multiple questions in parallel using batching.
    
    Args:
        questions_with_context: List of (file_path, text, question, frontmatter) tuples
        count: Number of paraphrases per question
    
    Returns:
        Dictionary mapping questions to their paraphrases
    """
    if not PARALLEL_PARAPHRASES:
        # Fall back to sequential processing
        results = {}
        for _, text, question, frontmatter in questions_with_context:
            answer = await generate_answer(question, text, frontmatter)
            paraphrases = await generate_related_questions_improved(question, answer, frontmatter, count)
            results[question] = paraphrases
        return results
    
    # First, generate answers for all questions
    answer_tasks = []
    for _, text, question, frontmatter in questions_with_context:
        answer_tasks.append(generate_answer(question, text, frontmatter))
    
    answers = await asyncio.gather(*answer_tasks)
    
    # Prepare question-answer pairs for batch processing
    qa_pairs = []
    for i, (_, _, question, frontmatter) in enumerate(questions_with_context):
        qa_pairs.append((question, answers[i], frontmatter))
    
    # Process in batches
    all_results = {}
    for i in range(0, len(qa_pairs), PARAPHRASE_BATCH_SIZE):
        batch = qa_pairs[i:i + PARAPHRASE_BATCH_SIZE]
        batch_results = await generate_batch_paraphrases(batch, count)
        all_results.update(batch_results)
    
    return all_results

async def generate_related_questions_improved(original_question: str, answer: str, frontmatter: dict, count: int = 2) -> list:
    """Improved version of generate_related_questions with better efficiency and quality.
    
    This version includes:
    - Caching support
    - Better prompt engineering
    - More reliable parsing
    - Semantic validation
    """
    # Check cache first
    cache_key = get_paraphrase_cache_key(original_question, answer)
    if cache_key in paraphrase_cache:
        return paraphrase_cache[cache_key][:count]
    
    title = frontmatter.get('title', '') or ''
    org_hint = ''
    if title:
        org_hint = f"Organization/context: '{title}'"

    prompt = f"""
    Generate paraphrased questions for a Q&A dataset.
    
    Original question: "{original_question}"
    Answer: {answer[:300]}{"..." if len(answer) > 300 else ""}
    {org_hint}
    
    Create EXACTLY {count} alternative questions that would lead to this same answer.
    
    VARIATION STRATEGIES TO USE:
    1. Syntactic: Change sentence structure (e.g., "What is X?" → "Can you explain X?")
    2. Lexical: Use synonyms (e.g., "CEO" → "chief executive", "contact" → "reach")
    3. Perspective: Different viewpoints (e.g., "How do I..." → "What's the process for...")
    
    REQUIREMENTS:
    - Each paraphrase must be distinct and natural
    - Include entity names when relevant
    - Keep questions concise (under 15 words ideally)
    - No meta phrases like "according to the text"
    - Must be answerable with the same answer
    
    OUTPUT:
    Return a JSON array with exactly {count} paraphrased questions.
    Example: ["Question 1?", "Question 2?"]
    
    JSON array:
    """

    output = await run_model(prompt)
    
    if DEBUG_PARAPHRASES:
        print(f"    ↳ Paraphrase output: {output[:300]}...")
    
    # Extract and clean paraphrases
    items = _extract_list_items(output)
    cleaned = []
    seen = set([original_question.strip().lower()])
    
    for q in items:
        qn = _normalize_question(q)
        key = qn.strip().lower()
        if qn and key not in seen:
            # Validate if enabled
            if VALIDATE_PARAPHRASES:
                similarity = calculate_similarity(original_question, qn)
                if 0.3 <= similarity <= 0.9:  # Not too similar, not too different
                    cleaned.append(qn)
                    seen.add(key)
                elif DEBUG_PARAPHRASES:
                    print(f"    ⚠ Rejected paraphrase (similarity={similarity:.2f}): {qn[:50]}...")
            else:
                cleaned.append(qn)
                seen.add(key)
        if len(cleaned) >= count:
            break
    
    # If we didn't get enough, try a simpler prompt
    if len(cleaned) < count:
        missing = count - len(cleaned)
        simple_prompt = f"""
        Create {missing} different ways to ask this question: "{original_question}"
        Return only a JSON array of strings.
        """
        output2 = await run_model(simple_prompt)
        items2 = _extract_list_items(output2)
        
        for q in items2:
            qn = _normalize_question(q)
            key = qn.strip().lower()
            if qn and key not in seen:
                cleaned.append(qn)
                seen.add(key)
            if len(cleaned) >= count:
                break
    
    # Cache the results
    paraphrase_cache[cache_key] = cleaned
    
    return cleaned

async def process_file(file_path, text, frontmatter, progress_counter, output_filepath, verbose=True, no_cache=False):
    """Process a file to extract questions and generate answers, writing results immediately."""
    questions_file_name = f"{file_path}.json"
    
    if not no_cache and Path(questions_file_name).is_file():
        with open(questions_file_name, 'r') as input_file:
            cached_data = json.loads(input_file.read())
            questions = []
            for item in cached_data:
                if len(item) == 3: # old format without frontmatter
                    questions.append(item + (frontmatter,))
                else:
                    questions.append(item)
    else:
        questions = await extract_questions_from_text(file_path, text, frontmatter, verbose)
        # Apply max_qa limit to the actual questions
        if len(questions) > MAX_QA_PAIRS:
            questions = questions[:MAX_QA_PAIRS]
            if verbose:
                print(f"  → Limited to {MAX_QA_PAIRS} questions (max_qa limit)")
        with open(questions_file_name, 'w') as output_file:
            json.dump(questions, output_file, indent=2)
    
    results_filename = f"{file_path}.result.json"
    result = []
    
    if not no_cache and Path(results_filename).is_file():
        with open(results_filename, 'r') as input_file:
            cached_result = json.loads(input_file.read())
        # Remove 'source' field from cached results if present
        result = []
        for item in cached_result:
            if GROUP_QUESTIONS:
                # Accept either new or old cached formats
                if 'questions' in item:
                    qa_pair = {
                        'questions': item.get('questions') or [],
                        'answer': item.get('answer')
                    }
                else:
                    qa_pair = {
                        'questions': [item.get('question')],
                        'answer': item.get('answer')
                    }
            else:
                qa_pair = {
                    'question': item.get('question') if 'question' in item else (item.get('questions') or [''])[0],
                    'answer': item.get('answer')
                }
            result.append(qa_pair)
        # Write cached results to main output file immediately
        append_results_to_file(result, output_filepath, verbose)
    else:
        if GROUP_QUESTIONS and PARALLEL_PARAPHRASES and len(questions) > 1:
            # Use parallel processing for paraphrases
            if verbose:
                print(f"  → Generating answers and paraphrases in parallel for {len(questions)} questions...")
            
            # Generate all paraphrases in parallel
            paraphrase_map = await generate_paraphrases_parallel(questions, PARAPHRASE_COUNT)
            
            # Now generate answers and combine with paraphrases
            for sub_file_path, sub_text, question, sub_frontmatter in questions:
                answer = await generate_answer(question, sub_text, sub_frontmatter)
                
                paraphrases = paraphrase_map.get(question, [])
                # Combine original with paraphrases
                all_qs = [question] + paraphrases
                
                qa_pair = {
                    'questions': all_qs,
                    'answer': answer
                }
                result.append(qa_pair)
                
                # Append this Q&A pair to the main output file immediately
                append_results_to_file([qa_pair], output_filepath, verbose)
        else:
            # Process questions one by one (original logic)
            for sub_file_path, sub_text, question, sub_frontmatter in questions:
                if verbose:
                    print(f"  → Generating answer for: {question[:80]}...")
                
                answer = await generate_answer(question, sub_text, sub_frontmatter)
                
                if GROUP_QUESTIONS:
                    try:
                        paraphrases = await generate_related_questions_improved(
                            question, answer, sub_frontmatter, count=PARAPHRASE_COUNT
                        )
                    except Exception as e:
                        if DEBUG_PARAPHRASES:
                            print(f"    ⚠ Paraphrase generation failed: {e}")
                        paraphrases = []
                    # Deduplicate and keep order: original first
                    all_qs = []
                    seen = set()
                    for q in [question] + paraphrases:
                        qn = q.strip()
                        if qn and qn.lower() not in seen:
                            seen.add(qn.lower())
                            all_qs.append(qn)
                    qa_pair = {
                        'questions': all_qs,
                        'answer': answer
                    }
                else:
                    qa_pair = {
                        'question': question,
                        'answer': answer
                    }
                result.append(qa_pair)
                
                # Append this Q&A pair to the main output file immediately
                append_results_to_file([qa_pair], output_filepath, verbose)
        
        # Save to individual result cache file
        with open(results_filename, 'w') as output_file:
            json.dump(result, output_file, indent=2)
    
    progress_counter['nb_files_done'] += 1
    if verbose:
        print(f"✓ {progress_counter['nb_files_done']}/{progress_counter['nb_files']}: File '{file_path}' completed ({len(result)} Q&A pairs)")
    
    return result

async def process_files(files, output_filepath, verbose=True, no_cache=False):
    """Process multiple files concurrently."""
    nb_files = len(files)
    progress_counter = {'nb_files': nb_files, 'nb_files_done': 0}
    
    if verbose:
        print(f"Starting question extraction on {nb_files} files.")
        if PARALLEL_PARAPHRASES and GROUP_QUESTIONS:
            print(f"  → Parallel paraphrase generation: ENABLED (batch size: {PARAPHRASE_BATCH_SIZE})")
        if VALIDATE_PARAPHRASES:
            print(f"  → Semantic validation: ENABLED")
    
    tasks = []
    for file_path, text, frontmatter in files:
        task = process_file(file_path, text, frontmatter, progress_counter, output_filepath, verbose, no_cache)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return flatten_nested_lists(results)


def parse_args():
    parser = argparse.ArgumentParser(description='Extract questions from documents (Improved Version)')
    parser.add_argument('--input', '-i', type=str, default='./data/docs',
                        help='Input directory containing documents (default: ./data/docs)')
    parser.add_argument('--mdfile', type=str,
                        help='Process a single markdown file instead of a directory')
    parser.add_argument('--output', '-o', type=str, default='./data/questions.json',
                        help='Output JSON file path (default: ./data/questions.json)')
    parser.add_argument('--quiet', '-q', action='store_true',
                        help='Run quietly with minimal output')
    parser.add_argument('--no-cache', action='store_true',
                        help='Skip cache and regenerate all questions and answers')
    parser.add_argument('--delay', type=float, default=0,
                        help='Delay in seconds between API calls (default: 0)')
    parser.add_argument('--concurrent', type=int, default=5,
                        help='Maximum concurrent API requests (default: 5)')
    parser.add_argument('--max-qa', type=int, default=300,
                        help='Maximum Q&A pairs per file (default: 300)')
    parser.add_argument('--batch-size', type=int, default=0,
                        help='Process files in batches of this size (0=all at once)')
    parser.add_argument('--no-warmup', action='store_true',
                        help='Skip model warm-up (default: False)')
    parser.add_argument('--timeout', type=int, default=300,
                        help='Request timeout in seconds (default: 300)')
    parser.add_argument('--test-connection', action='store_true',
                        help='Test Ollama connection and model availability without processing files')
    parser.add_argument('--group-questions', action='store_true',
                        help='Group multiple paraphrased questions for each answer')
    parser.add_argument('--paraphrases', type=int, default=2,
                        help='Number of paraphrased variants per original question (default: 2)')
    parser.add_argument('--paraphrase-batch-size', type=int, default=5,
                        help='Number of questions to paraphrase in a single API call (default: 5)')
    parser.add_argument('--no-parallel-paraphrases', action='store_true',
                        help='Disable parallel paraphrase generation')
    parser.add_argument('--no-validate-paraphrases', action='store_true',
                        help='Disable semantic validation of paraphrases')
    parser.add_argument('--debug-paraphrases', action='store_true',
                        help='Print raw paraphrase model outputs and parse details')
    return parser.parse_args()

async def cleanup_session():
    """Clean up the aiohttp session."""
    global session
    if session and not session.closed:
        await session.close()

async def main_async(args):
    """Async main function with optimizations."""
    # Update global variables with command-line arguments
    global MAX_QA_PAIRS, MAX_CONCURRENT_REQUESTS, API_DELAY, OLLAMA_TIMEOUT, throttler
    global GROUP_QUESTIONS, PARAPHRASE_COUNT, PARAPHRASE_BATCH_SIZE, PARALLEL_PARAPHRASES
    global VALIDATE_PARAPHRASES, DEBUG_PARAPHRASES
    
    MAX_QA_PAIRS = args.max_qa
    MAX_CONCURRENT_REQUESTS = args.concurrent
    API_DELAY = args.delay
    OLLAMA_TIMEOUT = args.timeout
    throttler = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    GROUP_QUESTIONS = bool(args.group_questions)
    PARAPHRASE_COUNT = max(0, int(args.paraphrases))
    PARAPHRASE_BATCH_SIZE = max(1, int(args.paraphrase_batch_size))
    PARALLEL_PARAPHRASES = not args.no_parallel_paraphrases
    VALIDATE_PARAPHRASES = not args.no_validate_paraphrases
    DEBUG_PARAPHRASES = bool(args.debug_paraphrases)
    
    try:
        # Warm up model unless disabled
        if not args.no_warmup:
            await warm_up_model()
        
        await run_extraction(args)
    except (ConnectionError, ValueError, RuntimeError) as e:
        print(f"\n❌ Fatal error: {e}")
        print("\nTroubleshooting tips:")
        if "Cannot connect" in str(e):
            print("  • Ensure Ollama is running: ollama serve")
            print("  • Check if the base_url in config.yaml is correct")
            print("  • Verify network connectivity to the Ollama server")
        elif "Model not found" in str(e):
            print("  • Install the model: ollama pull " + OLLAMA_MODEL)
            print("  • Check if the model name in config.yaml is correct")
            print("  • Verify the model is available: ollama list")
        elif "server error" in str(e):
            print("  • Restart Ollama: ollama serve")
            print("  • Check Ollama logs for errors")
            print("  • Verify the model is properly installed")
        raise
    finally:
        # Clean up HTTP session
        await cleanup_session()

async def run_extraction(args):
    """Run the main extraction logic."""
    
    output_filepath = Path(args.output)
    verbose = not args.quiet
    no_cache = args.no_cache
    batch_size = args.batch_size
    
    if verbose:
        if no_cache:
            print("Cache disabled. Regenerating all questions and answers.")
        print(f"Settings: delay={API_DELAY}s, concurrent={MAX_CONCURRENT_REQUESTS}, max_qa={MAX_QA_PAIRS}, timeout={OLLAMA_TIMEOUT}s")
        if GROUP_QUESTIONS:
            print(f"Question grouping: ON (paraphrases per question: {PARAPHRASE_COUNT})")
            if PARALLEL_PARAPHRASES:
                print(f"  → Parallel processing: ON (batch size: {PARAPHRASE_BATCH_SIZE})")
            if VALIDATE_PARAPHRASES:
                print(f"  → Semantic validation: ON")
        if batch_size > 0:
            print(f"Processing in batches of {batch_size} files")
    
    output_filepath.parent.mkdir(parents=True, exist_ok=True)
    
    # Initialize output file as empty JSON array
    with open(output_filepath, 'w', encoding='utf-8') as f:
        json.dump([], f)
    
    # Handle single file or directory input
    if args.mdfile:
        # Process single file
        file_path = Path(args.mdfile)
        if not file_path.exists():
            print(f"Error: File '{args.mdfile}' not found.")
            return
        
        if verbose:
            print(f"Processing single file: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                frontmatter, cleaned_body = parse_docling_markdown(content)
                
                # Add filename as source if not already present
                if 'source' not in frontmatter:
                    frontmatter['source'] = file_path.stem.replace('-', ' ').title()
                
                if len(cleaned_body.split()) > 20:
                    files = [(str(file_path), cleaned_body, frontmatter)]
                else:
                    print(f"File '{file_path}' has insufficient content after cleaning.")
                    return
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            return
    else:
        # Process directory
        input_directory = Path(args.input)
        if verbose:
            print(f"Loading and cleaning files from '{input_directory}'.")
        
        files = load_markdown_files_from_directory(input_directory)
        
        if not files:
            print(f"No suitable markdown files found in '{input_directory}'.")
            return
    
    all_results = []
    
    # Process in batches if specified
    if batch_size > 0:
        for i in range(0, len(files), batch_size):
            batch = files[i:i+batch_size]
            if verbose:
                print(f"\nProcessing batch {i//batch_size + 1}/{(len(files) + batch_size - 1)//batch_size}")
            
            batch_results = await process_files(batch, output_filepath, verbose, no_cache)
            all_results.extend(batch_results)
            
            if verbose:
                print(f"✓ Batch completed. {len(all_results)} total Q&A pairs generated so far")
    else:
        # Process all at once
        all_results = await process_files(files, output_filepath, verbose, no_cache)
    
    # Final status message
    if verbose:
        print(f"\n🎉 Done! {len(all_results)} question/answer pairs saved to {output_filepath}")
        print(f"💾 Results are being written in real-time - you can monitor progress by watching the file size of {output_filepath}")
        
        # Print cache statistics
        if paraphrase_cache:
            print(f"📊 Paraphrase cache: {len(paraphrase_cache)} entries cached")

def main():
    args = parse_args()
    
    
    # Handle test connection mode
    if args.test_connection:
        if test_ollama_connection():
            sys.exit(0)
        else:
            sys.exit(1)
    
    try:
        asyncio.run(main_async(args))
    except KeyboardInterrupt:
        print("\n\n⚠️  Process interrupted by user")
        sys.exit(1)
    except (ConnectionError, ValueError, RuntimeError) as e:
        print(f"\n❌ Fatal error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\n💥 Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()