# Orbit Server Dependencies Configuration
# This file defines dependency profiles for different use cases
# Start with minimal/basic and add profiles based on enabled adapters
#
# Usage:
#   Minimal server (default, Ollama included): ./setup.sh
#   Direct GGUF loading (requires C++ tools): ./setup.sh --profile llama-cpp
#   With cloud providers: ./setup.sh --profile providers
#   With file support: ./setup.sh --profile providers --profile files --profile embeddings
#   Recommended default: ./setup.sh --profile default
#   Full installation: ./setup.sh --profile all

# ============================================================================
# MINIMAL - Minimal server (supports API mode and Ollama, no direct GGUF loading)
# ============================================================================
[default]
description = "Minimal server dependencies (always installed - use API mode or Ollama for chat)"
dependencies = [
    "setuptools>=77.0.3,<80",
    "pydantic>=2.12.3",
    "jinja2>=3.1.6",
    "PyYAML>=6.0.2",
    "fastapi>=0.122.0",
    "uvicorn==0.38.0",
    "python-dotenv==1.0.1",
    "requests==2.32.5",
    "python-multipart>=0.0.14",
    "python-json-logger>=2.0.7",
    "aiohttp>=3.13.1",
    "aiodns>=3.4.0",
    "regex==2024.11.6",
    "sseclient-py==1.8.0",
    "psutil==7.0.0",
    "websockets>=15.0.1",
    "fastapi-mcp==0.4.0",
    "toml>=0.10.2",
    "rich>=14.1.0",
    "environs>=14.3.0",
    "prometheus-client>=0.23.1",
    "opentelemetry-proto==1.28.0",
    "schmitech-orbit-client==1.1.6",
    "tqdm>=4.66.2",
    "pymongo>=4.15.4",
    "redis>=7.1.0",  # Redis support for caching and sessions
    "elasticsearch==9.2.0",  # Elasticsearch support for intent adapters
    "ollama==0.6.1",  # Ollama local inference support
    "langchain-ollama>=1.0.0",
    "langchain-community>=0.4.1",
    "keyring>=25.6.0"
]

# ============================================================================
# LLAMA-CPP - Direct GGUF model loading with Python bindings
# ============================================================================
[profiles.llama-cpp]
description = "Direct GGUF model loading (llama-cpp-python for simple-chat adapter)"
dependencies = [
    "llama-cpp-python==0.3.16",
    "numpy<2.0.0",  # Required by llama-cpp-python
]

# ============================================================================
# PROVIDERS - Cloud inference providers (OpenAI, Anthropic, etc.)
# ============================================================================
[profiles.providers]
description = "Cloud inference providers (OpenAI, Anthropic, Cohere, etc.)"
dependencies = [
    "openai==2.6.1",
    "anthropic==0.71.0",
    "cohere==5.20.0",
    "groq==0.32.0",
    "replicate==1.0.7",
    "zai-sdk==0.0.4",
    "deepseek==1.0.0",
    "mistralai==1.9.11",
    "openrouter>=0.1.1",
    "google-generativeai==0.8.5",
    "together==1.5.29",
    "google-genai>=1.51.0",
]

# ============================================================================
# VECTOR-STORES - Vector database clients
# ============================================================================
[profiles.vector-stores]
description = "Vector store backends (ChromaDB, Pinecone, Qdrant, etc.)"
dependencies = [
    "chromadb>=1.3.5",
    "pinecone>=7.3.0",
    "qdrant-client>=1.16.0",
    "faiss-cpu==1.12.0",
    "weaviate-client==4.17.0",
    "pymilvus>=2.6.2",
    "marqo==3.18.0",
    "pgvector==0.4.1",
]

# ============================================================================
# DATABASES - SQL and NoSQL database connectors
# ============================================================================
[profiles.databases]
description = "Database connectors (PostgreSQL, MongoDB, MySQL, etc.)"
dependencies = [
    "motor>=3.7.1",
    "pymongo>=4.13.0",
    "psycopg2-binary>=2.9.10",
    "oracledb>=3.4.0",
    "mysql-connector-python>=9.4.0",
    "pymssql>=2.3.8",
    "cassandra-driver>=3.29.2",
    "duckdb>=1.4.1",
]

# ============================================================================
# FILES - File processing (PDF, DOCX, images, etc.)
# ============================================================================
[profiles.files]
description = "File processing for document Q&A (PDF, DOCX, PPTX, XLSX, CSV, VTT, images)"
dependencies = [
    "pypdf>=6.0.0",
    "python-docx>=1.2.0",
    "python-pptx>=1.0.2",
    "openpyxl>=3.1.5",
    "webvtt-py>=0.5.1",
    "pandas>=2.3.3",
    "beautifulsoup4>=4.14.1",
    "python-magic>=0.4.27",
    "filetype>=1.2.0",
    "docling>=2.64.0",
    "markitdown[all]>=0.1.4",
    "pyarrow>=22.0.0",
    "markdown>=3.9",
    "nh3>=0.3.0",
    "ftfy>=6.3.1",
    "unidecode>=1.3.8",
]

# ============================================================================
# EMBEDDINGS - Embedding models (sentence-transformers, etc.)
# ============================================================================
[profiles.embeddings]
description = "Embedding models for vector search"
dependencies = [
    "sentence-transformers>=5.1.2",
]

# ============================================================================
# AUDIO - Audio processing (Whisper, TTS, etc.)
# ============================================================================
[profiles.audio]
description = "Audio processing (Whisper transcription, TTS)"
dependencies = [
    "openai-whisper>=20250625",
    "snac>=1.2.1",  # SNAC codec for Orpheus TTS audio token decoding
]

# ============================================================================
# LANGUAGE-DETECTION - Language detection backends
# ============================================================================
[profiles.language-detection]
description = "Language detection backends (langdetect, langid, pycld2)"
dependencies = [
    "langdetect>=1.0.9",
    "langid>=1.1.6",
    "pycld2>=0.42",
    "pycountry>=24.6.1",
]

# ============================================================================
# FIRECRAWL - Firecrawl web scraping support
# ============================================================================
[profiles.firecrawl]
description = "Firecrawl web scraping for intent adapters"
dependencies = [
    "firecrawl==4.5.0",
]

# ============================================================================
# TORCH - PyTorch and local model inference (GPU recommended)
# ============================================================================
[profiles.torch]
description = "PyTorch and local model inference (GPU recommended)"
dependencies = [
    "numpy<2.0.0",  # Pin to NumPy 1.x for compatibility with PyTorch
    "torch==2.7.0",
    "vllm>=0.9.1",
    "transformers>=4.52.4",
    "accelerate>=0.27.2",
    "sentencepiece>=0.2.0",
    "protobuf>=5.27.2,<6.0.0"
]

# ============================================================================
# BITNET - BitNet 1.58-bit LLM inference framework
# ============================================================================
[profiles.bitnet]
description = "BitNet 1.58-bit LLM inference framework (requires building from source)"
dependencies = [
    # BitNet requires building from source - see docs/bitnet-setup.md
    "cmake>=3.15.0",
    "ninja>=1.10.0",
    "pybind11>=2.10.0",
    # Note: bitnet-cpp package is not available on PyPI yet
    # Users must build from: https://github.com/microsoft/BitNet
]

# ============================================================================
# CLOUD-SERVICES - Cloud platform services (AWS, Azure, GCP, IBM)
# ============================================================================
[profiles.cloud-services]
description = "Cloud platform services (AWS, Azure, GCP, IBM Watson)"
dependencies = [
    "boto3==1.40.55",
    "azure-ai-inference==1.0.0b9",
    "google-cloud-aiplatform==1.121.0",
    "ibm-watsonx-ai==1.4.1"
]

# ============================================================================
# DEFAULT - Recommended default for basic installation
# Includes minimal + common providers (Ollama included in minimal)
# ============================================================================
[profiles.default]
description = "Recommended default (minimal + providers)"
extends = ["providers"]
dependencies = []

# ============================================================================
# ALL - Everything (full installation)
# ============================================================================
[profiles.all]
description = "All available dependencies (full installation)"
extends = [
    "development",
    "llama-cpp",
    "providers",
    "vector-stores",
    "databases",
    "files",
    "embeddings",
    "language-detection",
    "firecrawl",
    "bitnet"
]
dependencies = []

# ============================================================================
# DEVELOPMENT - Development and testing tools
# ============================================================================
[profiles.development]
description = "Development and testing dependencies"
dependencies = [
    "pytest>=8.4.1",
    "pytest-asyncio>=1.1.0",
    "pytest-cov>=6.1.1",
    "black>=25.1.0",
    "flake8>=7.3.0",
    "mypy>=1.17.1",
    "faker>=37.5.3",
    "pre-commit>=4.2.0",
    "locust>=2.38.1",
    "reportlab>=4.4.4",
    "twine>=6.2.0",
    "build>=1.3.0"
]

# ============================================================================
# PROFILE USAGE EXAMPLES
# ============================================================================
# 
# Minimal server (default, use Ollama for local inference):
#   ./setup.sh
#   Note: Ollama is included by default - simplest option for new users
#
# Direct GGUF loading (for simple-chat adapter with local models):
#   ./setup.sh --profile llama-cpp
#   Note: Requires C++ build tools - use Ollama if you want simpler setup
#
# Recommended default (minimal + providers, Ollama included):
#   ./setup.sh --profile default
#
# Minimal + cloud providers:
#   ./setup.sh --profile providers
#
# Minimal + file support (for simple-chat-with-files adapter):
#   ./setup.sh --profile providers --profile files --profile embeddings --profile vector-stores
#
# With vector stores (if using ChromaDB, Pinecone, etc.):
#   ./setup.sh --profile vector-stores
#
# With databases (if using SQL/NoSQL adapters):
#   ./setup.sh --profile databases
#
# Full installation (everything):
#   ./setup.sh --profile all
#
# Custom combination example:
#   ./setup.sh --profile llama-cpp --profile providers --profile files --profile vector-stores
#
# Profile selection based on enabled adapters in default-config:
#   - simple-chat (Ollama): default (no additional profiles needed)
#   - simple-chat (direct GGUF): default + llama-cpp
#   - simple-chat-with-files: default + files + embeddings + vector-stores
#   - qa-sql: default + databases
#   - intent-elasticsearch: default (elasticsearch included)
#   - file-document-qa: default + files + embeddings + vector-stores
#
# llama-cpp profile (optional):
#   - Requires C++ compiler and build tools
#   - Use Ollama instead for simpler setup (included in default)
#   - Use API mode: Run llama.cpp server separately, no Python bindings needed
#
# ============================================================================