# Multimodal Adapters
# These adapters support multiple input types (text, images, audio files)
# with conditional retrieval based on file uploads

adapters:
  - name: "simple-chat-with-files"
    enabled: false
    type: "passthrough"  # Passthrough adapter with file retrieval capabilities
    datasource: "none"  # No actual datasource needed (files managed separately)
    adapter: "multimodal"
    implementation: "implementations.passthrough.multimodal.MultimodalImplementation"
    # Provider overrides (optional - use when you want different providers than global defaults)
    inference_provider: "ollama"    # Override default inference provider
    model: "granite4:1b"            # Specific model for this adapter
    embedding_provider: "ollama"          # Embedding provider for file chunk retrieval (matches uploaded files)
    # vision_provider: "ollama"             # Vision provider for image files: openai, gemini, anthropic (see vision.yaml)
    # stt_provider: "whisper"               # STT provider for audio transcription (see stt.yaml) - local, free!
    # tts_provider: "ollama"                # TTS provider for audio output (see tts.yaml)

    # Capabilities: Multimodal - conditional file retrieval
    capabilities:
      retrieval_behavior: "conditional"  # Only retrieves when file_ids are present
      formatting_style: "clean"          # Clean format (no citations) to prevent LLM citation markers
      supports_file_ids: true            # Accepts file_ids parameter for filtering
      supports_session_tracking: true    # Uses session_id for tracking
      requires_api_key_validation: true  # Validates file ownership via API key
      skip_when_no_files: true           # Skip retrieval when file_ids is empty
      optional_parameters:
        - "file_ids"
        - "api_key"
        - "session_id"

    config:
      # File adapter configuration (similar to file-document-qa)
      # Storage configuration
      storage_backend: "filesystem"  # Future: "s3", "minio"
      storage_root: "./uploads"
      max_file_size: 52428800  # 50MB

      # Processing configuration
      # Chunking strategy options: "fixed", "semantic", "token", "recursive"
      # These settings override global defaults in config.yaml
      chunking_strategy: "recursive"  # Options: "fixed", "semantic", "token", "recursive"
      chunk_size: 1000  # Increased to fit token-optimized CSV/JSON summaries in single chunks
      chunk_overlap: 100  # Characters for fixed/semantic, tokens for token/recursive

      # Vector store integration
      vector_store: "chroma"  # References stores.yaml
      collection_prefix: "files_"

      # Q&A settings - optimized for local LLMs with limited context (8K tokens)
      # 3 chunks × 2000 chars ≈ 6K chars, leaving room for system prompt + response
      confidence_threshold: 0.3
      max_results: 10       # Retrieve 5 chunks for good similarity matching
      return_results: 10    # Send top 3 to LLM to stay within context limits
