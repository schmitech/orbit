import:
  - "ollama.yaml"
  - "adapters.yaml"
  - "inference.yaml"
  - "datasources.yaml"
  - "embeddings.yaml"
  - "rerankers.yaml"
  - "stores.yaml"
  - "moderators.yaml"
  - "guardrails.yaml"
  - "vision.yaml"
  - "tts.yaml"
  - "stt.yaml"
  - "personaplex.yaml"

general:
  port: 3000
  https:
    enabled: false
    port: 3443
    cert_file: "./cert.pem"
    key_file: "./key.pem"
  session_id:
    header_name: "X-Session-ID"
    required: true
  inference_provider: "ollama"

language_detection:
  enabled: true
  backends:
    - "langdetect"
    - "langid"
    - "pycld2"
  backend_weights:
    langdetect: 1.0
    langid: 1.2
    pycld2: 1.5
  min_confidence: 0.7
  min_margin: 0.2
  prefer_english_for_ascii: true
  enable_stickiness: false
  fallback_language: "en"
  backend_timeout: 10.0  # Timeout per backend in seconds (increase for cold starts)

  # Configurable heuristic nudges for voting adjustments
  heuristic_nudges:
    en_boost: 0.2       # Boost for English in ASCII-heavy text
    es_penalty: 0.1     # Penalty for Spanish in pure ASCII (avoids es/en confusion)
    script_boost: 0.2   # Boost when script detection matches ensemble winner

  # Mixed language detection
  mixed_language_threshold: 0.3  # Min confidence for secondary language to flag as mixed

  # Chat history language prior (uses recent messages to stabilize detection)
  use_chat_history_prior: true
  chat_history_prior_weight: 0.3  # Weight given to chat history distribution
  chat_history_messages_count: 5  # Number of recent messages to consider

  # RAG retrieval language boosting (boosts documents matching user's language)
  retrieval_match_boost: 0.1       # Boost for documents matching detected language
  retrieval_mismatch_penalty: 0.05 # Penalty for non-matching language documents
  retrieval_min_confidence: 0.7    # Min detection confidence to apply boost

performance:
  workers: 4
  keep_alive_timeout: 30

  # Adapter preload timeout (seconds) - increase for Ollama cold starts
  # This is the maximum time to wait for each adapter to initialize during startup
  adapter_preload_timeout: 120  # 2 minutes (default was 60s)

  # GZip compression for responses (opt-in)
  # Compresses responses larger than minimum_size bytes (30-60% bandwidth savings for JSON)
  # Note: Streaming endpoints are automatically excluded to preserve word-by-word streaming
  # Recommended for deployments with large JSON responses on non-streaming endpoints
  compression:
    enabled: false  # Disabled by default - enable if needed for bandwidth optimization
    minimum_size: 2048  # Minimum response size in bytes to compress
    excluded_paths:     # Paths excluded from compression (streaming endpoints)
      - "/v1/chat"      # SSE streaming endpoint
      - "/ws"           # WebSocket endpoints
      - "/mcp"          # MCP protocol endpoints

  # ETag caching for GET requests (opt-in)
  # Returns 304 Not Modified for unchanged responses
  # Recommended for read-heavy REST API deployments, less useful for chat-focused use
  etag_caching:
    enabled: false  # Disabled by default - enable if clients implement ETag caching
    excluded_paths:
      - "/v1/chat"  # SSE streaming endpoint
      - "/ws"       # WebSocket endpoints
      - "/mcp"      # MCP protocol endpoints

  thread_pools:
    io_workers: 50              # Up from 10
    cpu_workers: 30             # CPU-bound tasks
    inference_workers: 20       # Model inference
    embedding_workers: 15       # Embedding generation
    db_workers: 25              # Database operations

fault_tolerance:
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout: 30
    success_threshold: 3
    timeout: 30
    max_recovery_timeout: 300.0
    enable_exponential_backoff: true
  execution:
    strategy: "all"  # "all", "first_success", "best_effort"
    timeout: 35 
    max_retries: 3
    retry_delay: 1
  
messages:
  no_results_response: "I'm sorry, but I don't have any specific information about that topic in my knowledge base."
  collection_not_found: "I couldn't find the requested collection. Please make sure the collection exists before querying it."

auth:
  session_duration_hours: 12
  default_admin_username: admin
  default_admin_password: ${ORBIT_DEFAULT_ADMIN_PASSWORD}
  pbkdf2_iterations: 600000
  # Credential storage method: "keyring" (default) or "file"
  # - keyring: Uses system keychain (macOS Keychain, Linux Secret Service)
  # - file: Uses plain text file in ~/.orbit/.env (less secure but visible)
  credential_storage: file

# Embedding configuration moved to embeddings.yaml

api_keys:
  header_name: "X-API-Key"
  prefix: "orbit_"

prompt_service:
  cache:
    ttl_seconds: 3600  # 1 hour - how long to cache prompts in Redis

logging:
  level: "INFO"
  handlers:
    file:
      enabled: true
      directory: "logs"
      filename: "orbit.log"
      max_size_mb: 10
      backup_count: 30
      rotation: "midnight"
      format: "text"
    console:
      enabled: false
      format: "text"
  capture_warnings: true
  propagate: false
  loggers:
    # Suppress all Python warnings (ResourceWarnings, DeprecationWarnings, etc.)
    py.warnings:
      level: "ERROR"
      propagate: false
      disabled: true
    # Reduce WebSocket message spam from inference server
    server.inference_server:
      level: "WARNING"
      propagate: false
    # Suppress uvicorn WebSocket connection logs
    uvicorn:
      level: "WARNING"
      propagate: false
    uvicorn.access:
      level: "WARNING"
      propagate: false
    uvicorn.asgi:
      level: "WARNING"
      propagate: false
    # Suppress Elasticsearch client transport logs
    elastic_transport:
      level: "WARNING"
      propagate: false
    elastic_transport.transport:
      level: "WARNING"
      propagate: false
    elasticsearch:
      level: "WARNING"
      propagate: false
    # Suppress Elasticsearch datasource connection logs
    datasources.implementations.elasticsearch_datasource:
      level: "WARNING"
      propagate: false
    inference.clients.llama_cpp:
      level: "ERROR"
    llama_cpp:
      level: "ERROR"
    llama_cpp.llama:
      level: "ERROR"
    ggml:
      level: "ERROR"
    metal:
      level: "ERROR"
    httpx:
      level: "WARNING"
    httpcore:
      level: "WARNING"
    hpack:
      level: "WARNING"
    docling:
      level: "WARNING"
    docling.datamodel.document:
      level: "WARNING"
    docling.document_converter:
      level: "WARNING"
    docling.pipeline.base_pipeline:
      level: "WARNING"
    docling.backend.msexcel_backend:
      level: "WARNING"
    docling.backend.pypdfium2_backend:
      level: "WARNING"
    docling.backend.docling_parse_backend:
      level: "WARNING"
    PIL.Image:
      level: "WARNING"
    pdfminer:
      level: "WARNING"
      propagate: false
    pdfminer.psparser:
      level: "WARNING"
      propagate: false
    pdfminer.pdfdocument:
      level: "WARNING"
      propagate: false
    pdfminer.pdfinterp:
      level: "WARNING"
      propagate: false

clock_service:
  enabled: true
  default_timezone: "America/Toronto"
  format: "%A, %B %d, %Y at %I:%M:%S %p %Z"

internal_services:
  # Backend database configuration
  # Choose between 'sqlite' (no installation required) or 'mongodb' (requires MongoDB server)
  backend:
    type: "sqlite" # sqlite or mongodb
    sqlite:
      database_path: "orbit.db"  # Path to SQLite database file (relative to project root)
  
  elasticsearch:
    enabled: false
    node: ${INTERNAL_SERVICES_ELASTICSEARCH_NODE}
    index: 'orbit'
    username: ${INTERNAL_SERVICES_ELASTICSEARCH_USERNAME}
    password: ${INTERNAL_SERVICES_ELASTICSEARCH_PASSWORD}

  # Audit trail storage configuration
  # Stores conversation audit logs for compliance and analytics
  audit:
    enabled: false
    # Storage backend for audit logs
    # Options: "elasticsearch", "sqlite", "mongodb", "database"
    # "database" means use the same backend as internal_services.backend.type
    storage_backend: "database"
    # Collection/table name for audit logs
    collection_name: "audit_logs"
    # Enable gzip compression for response field (saves storage, reduces I/O)
    # Set to false for debugging/testing to see plain text responses
    # Compressed responses are stored as base64-encoded gzip data
    compress_responses: false
    # Clear audit logs on server startup
    # WARNING: This will delete ALL audit logs when the server starts
    # Use with caution - only recommended for development/testing environments
    clear_on_startup: true

  mongodb:
    host: ${INTERNAL_SERVICES_MONGODB_HOST}
    port: ${INTERNAL_SERVICES_MONGODB_PORT}
    username: ${INTERNAL_SERVICES_MONGODB_USERNAME}
    password: ${INTERNAL_SERVICES_MONGODB_PASSWORD}
    database: ${INTERNAL_SERVICES_MONGODB_DB}
    users_collection: users
    sessions_collection: sessions
    apikey_collection: api_keys
    prompts_collection: system_prompts

  redis:
    enabled: false
    host: ${INTERNAL_SERVICES_REDIS_HOST}
    port: ${INTERNAL_SERVICES_REDIS_PORT}
    db: 0
    username: ${INTERNAL_SERVICES_REDIS_USERNAME}
    password: ${INTERNAL_SERVICES_REDIS_PASSWORD}
    use_ssl: false
    ttl: 3600  # 1 hour, matching temp_key_expiry

chat_history:
  enabled: true
  collection_name: "chat_history"
  store_metadata: true
  retention_days: 90
  max_tracked_sessions: 10000
  session:
    auto_generate: false
    required: true
    header_name: "X-Session-ID"
  user:
    header_name: "X-User-ID"
    required: false

conversation_threading:
  enabled: false
  dataset_ttl_hours: 24  # Global default TTL for stored datasets
  storage_backend: "sqlite"  # redis, sqlite, mongodb (fallback order: redis -> sqlite/mongodb)
  redis_key_prefix: "thread_dataset:"

# Autocomplete configuration for query suggestions based on intent template nl_examples
autocomplete:
  enabled: false  # Master switch - when false, /v1/autocomplete returns empty results

  # Query matching settings
  min_query_length: 3  # Minimum characters before fetching suggestions
  max_suggestions: 500   # Maximum suggestions returned per request

  # Caching configuration
  cache:
    # Use Redis for distributed caching (recommended for multi-instance deployments)
    # Falls back to in-memory cache if Redis is unavailable
    use_redis: true
    ttl_seconds: 1800  # 30 minutes - templates rarely change
    redis_key_prefix: "autocomplete:"

  # Fuzzy matching configuration
  fuzzy_matching:
    enabled: true  # Enable fuzzy/approximate string matching
    # Algorithm options: "levenshtein", "jaro_winkler", "substring" (default)
    # - substring: Exact substring matching (fastest, no typo tolerance)
    # - levenshtein: Edit distance (handles typos, moderate speed)
    # - jaro_winkler: Optimized for short strings and prefixes (good for typos)
    algorithm: "jaro_winkler"
    # Minimum similarity score (0.0-1.0) to include a suggestion
    # Higher = stricter matching, fewer results
    # Recommended: 0.7 for levenshtein, 0.8 for jaro_winkler
    threshold: 0.75
    # Maximum candidates to evaluate with fuzzy matching (performance guard)
    max_candidates: 1000

# Composite Intent Retriever template selection configuration
# Improves template selection accuracy when routing queries across multiple intent adapters
# by using multi-stage scoring with reranking and string similarity
composite_retrieval:
  # Two-stage retrieval with reranking
  # Uses the configured reranker (from rerankers.yaml) to re-score top candidates
  reranking:
    enabled: false              # Enable reranker stage for better semantic understanding
    provider: "ollama"      # Reranker provider from rerankers.yaml (anthropic, cohere, openai, etc.)
    top_candidates: 10         # Number of embedding candidates to pass to reranker
    weight: 0.4                # Weight for reranker score in final combined score

  # String similarity scoring
  # Adds lexical matching to complement semantic embeddings
  string_similarity:
    enabled: true              # Enable string similarity scoring
    # Algorithm options:
    # - "jaro_winkler": Best for short strings and typo tolerance (recommended)
    # - "levenshtein": Edit distance, good for detecting insertions/deletions
    # - "ratio": Simple ratio matching (fastest)
    algorithm: "jaro_winkler"
    weight: 0.2                # Weight for string similarity in final combined score
    # Template fields to compare against query for similarity
    compare_fields:
      - "description"          # Template description
      - "nl_examples"          # Natural language examples (primary matching target)
    # Minimum string similarity to consider (0.0-1.0)
    # Results below this are penalized
    min_threshold: 0.3
    # Aggregate multiple field scores: "max", "avg", "weighted_avg"
    aggregation: "max"

  # Combined scoring formula:
  # final_score = (embedding_weight * emb) + (rerank_weight * rerank) + (string_weight * str_sim)
  # Weights should sum to 1.0 for normalized scoring
  scoring:
    embedding_weight: 0.4      # Weight for embedding similarity (base retrieval)
    normalize_scores: true     # Normalize all scores to 0-1 before combining
    # Tie-breaker strategy when scores are very close (within 0.01)
    tie_breaker: "embedding"   # Options: "embedding", "reranker", "string_similarity"

  # Performance settings
  performance:
    parallel_rerank: true      # Rerank candidates in parallel batches
    cache_rerank_results: true # Cache reranking results for repeated queries
    cache_ttl_seconds: 300     # TTL for reranking cache (5 minutes)

files:
  # Global defaults for all file adapters
  # Individual adapters in adapters.yaml can override these settings

  # NOTE: File metadata (uploaded_files, file_chunks) is now stored in the main backend database
  # configured in internal_services.backend (SQLite or MongoDB), not in a separate files.db

  # Default storage configuration (can be overridden per adapter)
  storage_root: "./uploads"  # Default root directory for storing uploaded files
  
  # Default processing settings (can be overridden per adapter)
  default_chunking_strategy: "recursive"  # Options: "fixed", "semantic", "token", "recursive"
  # Recommended: "recursive" - works best for all file types (PDF, DOCX, CSV, TXT, HTML, JSON, images, audio)
  # Respects document structure (paragraphs → sentences → words) while handling structured data
  default_chunk_size: 2048  # Default chunk size (characters for fixed/semantic, tokens for token/recursive)
  default_chunk_overlap: 200  # Default overlap (characters for fixed/semantic, tokens for token/recursive)
  
  # Processing configuration
  processing:
    # Docling processor configuration
    # Set to false to disable docling processor and prevent outbound connections to HuggingFace at startup
    # When disabled, docling will not be initialized until actually needed (lazy initialization)
    # Default: true (enabled)
    docling_enabled: false

    # MarkItDown processor configuration (Microsoft's document-to-markdown converter)
    # Set to true to enable MarkItDown as an alternative/fallback document processor
    # Supports: PDF, DOCX, PPTX, XLSX, XLS, HTML, CSV, JSON, XML, images, audio, ZIP, EPUB
    # Requires: pip install markitdown[all]
    # Default: false (opt-in)
    markitdown_enabled: false

    # Processor priority when multiple universal processors are enabled
    # Determines which processor is tried first for overlapping MIME types
    # Options: "docling" (default), "markitdown", "native"
    # - docling: Try Docling first (advanced document understanding)
    # - markitdown: Try MarkItDown first (clean markdown conversion)
    # - native: Only use native Python processors (no universal processors)
    processor_priority: "docling"

    # MarkItDown-specific options
    markitdown:
      # Enable third-party plugins (default: false for security)
      enable_plugins: false

    # CSV processor settings
    csv:
      # For small CSVs, include ALL rows to enable exact lookups (e.g., "find ID X")
      # Files with <= this many rows will have all data indexed, not just samples
      # Set to 0 to always use summary mode (token-efficient but no exact lookups)
      full_data_row_threshold: 200

      # Number of sample rows to show in summary mode (for large files)
      max_preview_rows: 5

      # Maximum characters per column value before truncation
      max_column_width: 50

      # Maximum columns to show in detail (remaining columns listed by name only)
      max_columns_full: 15

    # JSON processor settings
    json:
      # For small JSON arrays, include ALL items to enable exact lookups (e.g., "find ID X")
      # Arrays with <= this many items will have all data indexed, not just samples
      # Set to 0 to always use summary mode (token-efficient but no exact lookups)
      full_data_item_threshold: 200

      # Maximum array items to show in previews (for large arrays in summary mode)
      max_array_preview_items: 3

      # Maximum depth for schema extraction
      max_schema_depth: 4

      # Maximum string length before truncation
      max_string_length: 100

      # Maximum keys to show for large objects
      max_object_keys: 20

  # Tokenizer configuration (optional)
  # If not specified, uses character-based tokenization
  # Options: "character" (default), "gpt2", "tiktoken", or custom tokenizer identifier
  # Requires chonkie library for advanced tokenizers
  tokenizer: null  # e.g., "gpt2", "tiktoken", or null for character-based
  
  # Token-based chunking option for fixed strategy
  # If true, uses token-based chunking instead of character-based
  use_tokens: false
  
  # Strategy-specific chunking options
  chunking_options:
    # Semantic chunking options
    model_name: null  # Optional sentence-transformer model name (e.g., "all-MiniLM-L6-v2")
    use_advanced: false  # Enable advanced semantic chunking with similarity calculations (requires sentence-transformers)
    chunk_size_tokens: null  # Optional token-based chunk size limit for semantic chunks
    
    # Recursive chunking options
    min_characters_per_chunk: 24  # Minimum characters per chunk for recursive chunking
    
    # Advanced semantic chunking options (when use_advanced: true)
    threshold: 0.8  # Similarity threshold (0-1) for semantic boundary detection
    similarity_window: 3  # Number of sentences to consider for similarity calculation
    min_sentences_per_chunk: 1  # Minimum sentences per chunk
    min_characters_per_sentence: 24  # Minimum characters per sentence
    skip_window: 0  # Number of groups to skip when merging (0=disabled)
    filter_window: 5  # Window length for Savitzky-Golay filter (requires scipy)
    filter_polyorder: 3  # Polynomial order for Savitzky-Golay filter
    filter_tolerance: 0.2  # Tolerance for Savitzky-Golay filter
  
  # Default vector store settings (can be overridden per adapter)
  default_vector_store: "chroma"  # Default vector store to use
  default_collection_prefix: "files_"  # Default prefix for collection names

monitoring:
  enabled: true                    # Enable/disable monitoring dashboard
  metrics:
    collection_interval: 5         # Seconds between metric collections
    time_window: 300              # Seconds of historical data to keep (5 minutes)
    prometheus:
      enabled: true               # Enable Prometheus endpoint at /metrics
    dashboard:
      enabled: true               # Enable web dashboard at /dashboard
      websocket_update_interval: 5 # Seconds between WebSocket updates
  alerts:                         # Alert thresholds (future use)
    cpu_threshold: 90             # Alert when CPU > 90%
    memory_threshold: 85          # Alert when memory > 85%
    error_rate_threshold: 5       # Alert when error rate > 5%
    response_time_threshold: 5000 # Alert when avg response time > 5000ms

security:
  # CORS (Cross-Origin Resource Sharing) configuration
  cors:
    # Allowed origins - MUST be explicitly configured for production
    # Use specific origins like ["https://app.example.com", "https://admin.example.com"]
    # Use ["*"] ONLY for development (credentials will be automatically disabled)
    allowed_origins: ["*"]  # WARNING: Wildcard only for development

    # Allow credentials (cookies, authorization headers)
    # IMPORTANT: Cannot be true when allowed_origins contains "*"
    # Set to true only when using specific origin list
    allow_credentials: false

    # Allowed HTTP methods
    allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"]

    # Allowed request headers
    allowed_headers:
      - "Authorization"
      - "Content-Type"
      - "X-API-Key"
      - "X-Session-ID"
      - "X-User-ID"
      - "X-Request-ID"

    # Headers exposed to the browser
    expose_headers:
      - "X-Request-ID"
      - "X-RateLimit-Limit"
      - "X-RateLimit-Remaining"
      - "X-RateLimit-Reset"

    # Max age for preflight request caching (seconds)
    max_age: 600

  # Security headers configuration
  headers:
    enabled: true

    # Content Security Policy - restricts resource loading
    # Adjust based on your frontend requirements
    # Allows dashboard external resources: Chart.js CDN and Google Fonts
    content_security_policy: "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; img-src 'self' data: blob:; connect-src 'self' ws: wss:; font-src 'self' data: https://fonts.gstatic.com https://fonts.googleapis.com; frame-ancestors 'self'"

    # HTTP Strict Transport Security - enforce HTTPS
    # Only effective when served over HTTPS
    strict_transport_security: "max-age=31536000; includeSubDomains"

    # Prevent MIME type sniffing
    x_content_type_options: "nosniff"

    # Clickjacking protection
    x_frame_options: "SAMEORIGIN"

    # XSS protection (legacy, but still useful)
    x_xss_protection: "1; mode=block"

    # Referrer policy - control referrer information
    referrer_policy: "strict-origin-when-cross-origin"

    # Permissions policy - control browser features
    permissions_policy: "geolocation=(), microphone=(), camera=()"

  # Request size limits
  request_limits:
    max_body_size_mb: 10  # Maximum request body size in MB

  # Error handling security
  error_handling:
    # Never expose detailed errors in production
    # Set to true only for development/debugging
    expose_details: true

  # Rate limiting configuration (requires Redis to be enabled)
  rate_limiting:
    enabled: false                    # Master switch (requires Redis to be enabled)

    # Proxy header trust configuration
    # SECURITY: Only enable trust_proxy_headers when behind a trusted reverse proxy
    # When false (default), X-Forwarded-For and X-Real-IP headers are ignored
    # to prevent IP spoofing attacks that bypass rate limiting
    trust_proxy_headers: false

    # Optional: List of trusted proxy IPs/CIDRs that are allowed to set proxy headers
    # Only used when trust_proxy_headers is true
    # If empty, all proxy headers are trusted (use only in controlled environments)
    trusted_proxies: []
    # Example trusted_proxies configuration:
    # trusted_proxies:
    #   - "10.0.0.0/8"        # Private network
    #   - "172.16.0.0/12"     # Private network
    #   - "192.168.0.0/16"    # Private network
    #   - "127.0.0.1"         # Localhost

    # IP-based limits (applies to all requests)
    ip_limits:
      requests_per_minute: 60        # Max requests per IP per minute
      requests_per_hour: 1000        # Max requests per IP per hour

    # API key limits (higher limits for authenticated requests)
    api_key_limits:
      requests_per_minute: 120       # Max requests per API key per minute
      requests_per_hour: 5000        # Max requests per API key per hour

    # Paths to exclude from rate limiting
    exclude_paths:
      - "/health"
      - "/favicon.ico"
      - "/metrics"
      - "/static"

    # Response configuration
    retry_after_seconds: 60          # Retry-After header value when limited

  # Throttling configuration (executes BEFORE rate limiting)
  # Delays requests progressively instead of rejecting them
  throttling:
    enabled: false                    # Master switch for throttling

    # Default quotas for API keys (can be overridden per-key)
    default_quotas:
      daily_limit: 10000             # Default daily request limit
      monthly_limit: 100000          # Default monthly request limit

    # Delay calculation parameters
    delay:
      min_ms: 100                    # Minimum delay when throttling starts
      max_ms: 5000                   # Maximum delay before quota rejection
      curve: "exponential"           # "linear" or "exponential"
      threshold_percent: 70          # Start throttling at 70% of quota

    # Priority-based delay multipliers (priority 1-10)
    # Higher multiplier = more delay for that priority level
    priority_multipliers:
      1: 0.5                         # Premium tier: half delay
      5: 1.0                         # Standard tier: normal delay
      10: 2.0                        # Low priority: double delay

    # Paths to exclude from throttling
    exclude_paths: []

    # Redis configuration
    redis_key_prefix: "quota:"
    usage_sync_interval_seconds: 60  # Sync Redis usage to database

    # Response headers
    headers:
      delay: "X-Throttle-Delay"
      daily_remaining: "X-Quota-Daily-Remaining"
      monthly_remaining: "X-Quota-Monthly-Remaining"
      daily_reset: "X-Quota-Daily-Reset"
      monthly_reset: "X-Quota-Monthly-Reset"
