FROM python:3.12-slim

WORKDIR /orbit

# ═══════════════════════════════════════════════════════════════════════════
# Performance Optimizations for CPU/GPU inference
# ═══════════════════════════════════════════════════════════════════════════
# - OpenBLAS: Optimized linear algebra for CPU inference
# - numactl: NUMA-aware memory allocation for multi-socket systems
# - Environment variables optimize threading and memory for ML workloads
# ═══════════════════════════════════════════════════════════════════════════

# Install system dependencies including performance libraries
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    git \
    gcc \
    g++ \
    make \
    libssl-dev \
    libffi-dev \
    libpq-dev \
    zstd \
    # Performance optimizations
    libopenblas-dev \
    numactl \
    # GPU detection utilities (works without NVIDIA driver)
    pciutils \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# ═══════════════════════════════════════════════════════════════════════════
# CPU Performance Environment Variables
# ═══════════════════════════════════════════════════════════════════════════
# These optimize threading and memory allocation for inference workloads
ENV OMP_NUM_THREADS=4 \
    MKL_NUM_THREADS=4 \
    OPENBLAS_NUM_THREADS=4 \
    VECLIB_MAXIMUM_THREADS=4 \
    NUMEXPR_NUM_THREADS=4 \
    # Reduce Python memory fragmentation
    MALLOC_TRIM_THRESHOLD_=100000 \
    # Optimize for inference (not training)
    TOKENIZERS_PARALLELISM=false

# Install Node.js (LTS version) for orbitchat web app
RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
    apt-get install -y nodejs && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install orbitchat globally
RUN npm install -g orbitchat

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Copy dependency configuration first (for better caching)
COPY install/dependencies.toml /orbit/install/
COPY install/generate_requirements.py /orbit/install/

# Copy setup script and make it executable
COPY install/setup.sh /orbit/install/
RUN chmod +x /orbit/install/setup.sh

# Install Python dependencies (default profile only - minimal for basic chat)
RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir tomli && \
    cd /orbit/install && \
    python3 generate_requirements.py default --output /tmp/requirements.txt && \
    pip install --no-cache-dir -r /tmp/requirements.txt && \
    rm /tmp/requirements.txt

# Install additional dependencies for basic server functionality
# Pillow is required by ai_services/vision_service (imported even if not used)
RUN pip install --no-cache-dir requests tqdm pyyaml Pillow httpx

# Copy the application code
COPY server/ /orbit/server/
COPY bin/ /orbit/bin/
COPY LICENSE* /orbit/

# Copy default-config as config directory (same as tarball approach)
# This matches exactly how build-tarball.sh copies config files
# Includes ollama.yaml which is referenced by inference.yaml
COPY install/default-config/ /orbit/config/

# ═══════════════════════════════════════════════════════════════════════════
# Docker Config Profiles
# ═══════════════════════════════════════════════════════════════════════════
# Available profiles in docker/config/:
#   - config.yaml.minimal    : Basic chat only (smallest image, ~2GB)
#   - config.yaml.multimodal : Files, vision, audio support (future)
#   - config.yaml.full       : All features enabled (future)
# ═══════════════════════════════════════════════════════════════════════════
COPY docker/config/config.yaml.minimal /orbit/config/config.yaml
COPY docker/config/adapters/passthrough.yaml.minimal /orbit/config/adapters/passthrough.yaml

# Create .env file from env.example (same as tarball approach)
COPY env.example /orbit/.env

# Make scripts executable
RUN chmod +x /orbit/bin/orbit.py /orbit/bin/orbit.sh

# Create necessary directories
RUN mkdir -p /orbit/logs /orbit/data /orbit/models

# Copy default database file (so user won't need to create a default key)
COPY install/orbit.db.default /orbit/orbit.db

# ═══════════════════════════════════════════════════════════════════════════
# Pull Models During Build
# ═══════════════════════════════════════════════════════════════════════════
# SmolLM2 1.7B: Ultra-fast inference model (~1.2GB) - optimized for edge
# nomic-embed-text: Embedding model for RAG/search functionality
# ═══════════════════════════════════════════════════════════════════════════
RUN ollama serve & \
    sleep 5 && \
    ollama pull smollm2 && \
    ollama pull nomic-embed-text:latest && \
    pkill ollama || true

# Set environment variables
ENV PYTHONPATH=/orbit:/orbit/server
ENV PATH="/orbit/bin:${PATH}"
ENV OLLAMA_HOST=0.0.0.0:11434

# ═══════════════════════════════════════════════════════════════════════════
# GPU/CPU Auto-Detection Configuration
# ═══════════════════════════════════════════════════════════════════════════
# ORBIT_PRESET: Override automatic GPU detection (optional)
#   - "auto" (default): Detect GPU and select smollm2-1.7b-gpu or smollm2-1.7b-cpu
#   - "smollm2-1.7b-gpu": Force GPU preset
#   - "smollm2-1.7b-cpu": Force CPU preset
#   - Any other preset name from ollama.yaml
# ═══════════════════════════════════════════════════════════════════════════
ENV ORBIT_PRESET=auto

# Expose the web app port and API port (Ollama runs internally)
EXPOSE 5173 3000

# Health check - check both the API and web app
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
  CMD curl -f http://localhost:3000/health && curl -f http://localhost:5173 || exit 1

# Copy entrypoint script
COPY docker/docker-entrypoint-basic.sh /orbit/docker-entrypoint-basic.sh
RUN chmod +x /orbit/docker-entrypoint-basic.sh

# Run the entrypoint script
ENTRYPOINT ["/orbit/docker-entrypoint-basic.sh"]
